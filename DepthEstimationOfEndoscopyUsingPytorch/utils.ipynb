{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from plyfile import PlyData, PlyElement\n",
        "import yaml\n",
        "import random\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "import datetime\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "LY3mjCTD86lF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plyfile"
      ],
      "metadata": {
        "id": "lJxmIbGp9WTh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e4e5fb-0b23-4ff4-887b-88854ee9cb73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plyfile\n",
            "  Downloading plyfile-0.9-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.22.4)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib --upgrade"
      ],
      "metadata": {
        "id": "V07zJWJU9xMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlapping_visible_view_indexes_per_point(visible_view_indexes_per_point, visible_interval):\n",
        "    temp_array = np.copy(visible_view_indexes_per_point)\n",
        "    view_count = visible_view_indexes_per_point.shape[1]\n",
        "    for i in range(view_count):\n",
        "        visible_view_indexes_per_point[:, i] = \\\n",
        "            np.sum(temp_array[:, max(0, i - visible_interval):min(view_count, i + visible_interval)], axis=1)\n",
        "\n",
        "    return visible_view_indexes_per_point"
      ],
      "metadata": {
        "id": "VJAUnlTg9Bwp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_color_file_names_by_bag(root, training_patient_id, validation_patient_id, testing_patient_id):\n",
        "    training_image_list = []\n",
        "    validation_image_list = []\n",
        "    testing_image_list = []\n",
        "\n",
        "    if not isinstance(training_patient_id, list):\n",
        "        training_patient_id = [training_patient_id]\n",
        "    if not isinstance(validation_patient_id, list):\n",
        "        validation_patient_id = [validation_patient_id]\n",
        "    if not isinstance(testing_patient_id, list):\n",
        "        testing_patient_id = [testing_patient_id]\n",
        "\n",
        "    for id in training_patient_id:\n",
        "        training_image_list += list(root.glob('*' + str(id) + '/_start*/0*.jpg'))\n",
        "    for id in testing_patient_id:\n",
        "        testing_image_list += list(root.glob('*' + str(id) + '/_start*/0*.jpg'))\n",
        "    for id in validation_patient_id:\n",
        "        validation_image_list += list(root.glob('*' + str(id) + '/_start*/0*.jpg'))\n",
        "\n",
        "    training_image_list.sort()\n",
        "    testing_image_list.sort()\n",
        "    validation_image_list.sort()\n",
        "    return training_image_list, validation_image_list, testing_image_list\n",
        "\n",
        "\n",
        "def get_color_file_names(root, split_ratio=(0.9, 0.05, 0.05)):\n",
        "    image_list = list(root.glob('*/_start*/0*.jpg'))\n",
        "    image_list.sort()\n",
        "    split_point = [int(len(image_list) * split_ratio[0]), int(len(image_list) * (split_ratio[0] + split_ratio[1]))]\n",
        "    return image_list[:split_point[0]], image_list[split_point[0]:split_point[1]], image_list[split_point[1]:]\n",
        "\n",
        "\n",
        "def get_test_color_img(img_file_name, start_h, end_h, start_w, end_w, downsampling_factor, is_hsv, rgb_mode):\n",
        "    img = cv2.imread(img_file_name)\n",
        "    downsampled_img = cv2.resize(img, (0, 0), fx=1. / downsampling_factor, fy=1. / downsampling_factor)\n",
        "    downsampled_img = downsampled_img[start_h:end_h, start_w:end_w, :]\n",
        "    if is_hsv:\n",
        "        downsampled_img = cv2.cvtColor(downsampled_img, cv2.COLOR_BGR2HSV_FULL)\n",
        "    else:\n",
        "        if rgb_mode == \"rgb\":\n",
        "            downsampled_img = cv2.cvtColor(downsampled_img, cv2.COLOR_BGR2RGB)\n",
        "    downsampled_img = np.array(downsampled_img, dtype=\"float32\")\n",
        "    return downsampled_img\n",
        "\n",
        "\n",
        "def get_parent_folder_names(root, id_range):\n",
        "    folder_list = []\n",
        "    for i in range(id_range[0], id_range[1]):\n",
        "        folder_list += list(root.glob('*' + str(i) + '/_start*/'))\n",
        "\n",
        "    folder_list.sort()\n",
        "    return folder_list\n",
        "\n",
        "\n",
        "def downsample_and_crop_mask(mask, downsampling_factor, divide, suggested_h=None, suggested_w=None):\n",
        "    downsampled_mask = cv2.resize(mask, (0, 0), fx=1. / downsampling_factor, fy=1. / downsampling_factor)\n",
        "    end_h_index = downsampled_mask.shape[0]\n",
        "    end_w_index = downsampled_mask.shape[1]\n",
        "    # divide is related to the pooling times of the teacher model\n",
        "    indexes = np.where(downsampled_mask == 255)\n",
        "    h = indexes[0].max() - indexes[0].min()\n",
        "    w = indexes[1].max() - indexes[1].min()\n",
        "\n",
        "    remainder_h = h % divide\n",
        "    remainder_w = w % divide\n",
        "\n",
        "    increment_h = divide - remainder_h\n",
        "    increment_w = divide - remainder_w\n",
        "\n",
        "    target_h = h + increment_h\n",
        "    target_w = w + increment_w\n",
        "\n",
        "    start_h = max(indexes[0].min() - increment_h // 2, 0)\n",
        "    end_h = start_h + target_h\n",
        "\n",
        "    start_w = max(indexes[1].min() - increment_w // 2, 0)\n",
        "    end_w = start_w + target_w\n",
        "\n",
        "    if suggested_h is not None:\n",
        "        if suggested_h != h:\n",
        "            remain_h = suggested_h - target_h\n",
        "            start_h = max(start_h - remain_h // 2, 0)\n",
        "            end_h = min(suggested_h + start_h, end_h_index)\n",
        "            start_h = end_h - suggested_h\n",
        "\n",
        "    if suggested_w is not None:\n",
        "        if suggested_w != w:\n",
        "            remain_w = suggested_w - target_w\n",
        "            start_w = max(start_w - remain_w // 2, 0)\n",
        "            end_w = min(suggested_w + start_w, end_w_index)\n",
        "            start_w = end_w - suggested_w\n",
        "\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    downsampled_mask_erode = cv2.erode(downsampled_mask, kernel, iterations=1)\n",
        "    cropped_mask = downsampled_mask_erode[start_h:end_h, start_w:end_w]\n",
        "    return cropped_mask, start_h, end_h, start_w, end_w\n",
        "\n",
        "\n",
        "def read_selected_indexes(prefix_seq):\n",
        "    selected_indexes = []\n",
        "    with open(str(prefix_seq / 'selected_indexes')) as fp:\n",
        "        for line in fp:\n",
        "            selected_indexes.append(int(line))\n",
        "\n",
        "    stride = selected_indexes[1] - selected_indexes[0]\n",
        "    return stride, selected_indexes\n",
        "\n",
        "\n",
        "def read_visible_image_path_list(data_root):\n",
        "    visible_image_path_list = []\n",
        "    visible_indexes_path_list = list(data_root.rglob(\"*visible_view_indexes\"))\n",
        "    for index_path in visible_indexes_path_list:\n",
        "        with open(str(index_path)) as fp:\n",
        "            for line in fp:\n",
        "                visible_image_path_list.append(int(line))\n",
        "    return visible_image_path_list\n",
        "\n",
        "\n",
        "def read_visible_view_indexes(prefix_seq):\n",
        "    visible_view_indexes = []\n",
        "    with open(str(prefix_seq / 'visible_view_indexes')) as fp:\n",
        "        for line in fp:\n",
        "            visible_view_indexes.append(int(line))\n",
        "\n",
        "    return visible_view_indexes\n",
        "\n",
        "\n",
        "def read_camera_intrinsic_per_view(prefix_seq):\n",
        "    camera_intrinsics = []\n",
        "    param_count = 0\n",
        "    temp_camera_intrincis = np.zeros((3, 4))\n",
        "    with open(str(prefix_seq / 'camera_intrinsics_per_view')) as fp:\n",
        "        for line in fp:\n",
        "            # Focal length\n",
        "            if param_count == 0:\n",
        "                temp_camera_intrincis[0][0] = float(line)\n",
        "                param_count += 1\n",
        "            elif param_count == 1:\n",
        "                temp_camera_intrincis[1][1] = float(line)\n",
        "                param_count += 1\n",
        "            elif param_count == 2:\n",
        "                temp_camera_intrincis[0][2] = float(line)\n",
        "                param_count += 1\n",
        "            elif param_count == 3:\n",
        "                temp_camera_intrincis[1][2] = float(line)\n",
        "                temp_camera_intrincis[2][2] = 1.0\n",
        "                camera_intrinsics.append(temp_camera_intrincis)\n",
        "                temp_camera_intrincis = np.zeros((3, 4))\n",
        "                param_count = 0\n",
        "    return camera_intrinsics\n",
        "\n",
        "\n",
        "def modify_camera_intrinsic_matrix(intrinsic_matrix, start_h, start_w, downsampling_factor):\n",
        "    intrinsic_matrix_modified = np.copy(intrinsic_matrix)\n",
        "    intrinsic_matrix_modified[0][0] = intrinsic_matrix[0][0] / downsampling_factor\n",
        "    intrinsic_matrix_modified[1][1] = intrinsic_matrix[1][1] / downsampling_factor\n",
        "    intrinsic_matrix_modified[0][2] = intrinsic_matrix[0][2] / downsampling_factor - start_w\n",
        "    intrinsic_matrix_modified[1][2] = intrinsic_matrix[1][2] / downsampling_factor - start_h\n",
        "    return intrinsic_matrix_modified\n",
        "\n",
        "\n",
        "def read_point_cloud(path):\n",
        "    lists_3D_points = []\n",
        "    plydata = PlyData.read(path)\n",
        "    for n in range(plydata['vertex'].count):\n",
        "        temp = list(plydata['vertex'][n])\n",
        "        temp[0] = temp[0]\n",
        "        temp[1] = temp[1]\n",
        "        temp[2] = temp[2]\n",
        "        temp.append(1.0)\n",
        "        lists_3D_points.append(temp)\n",
        "    return lists_3D_points\n",
        "\n",
        "\n",
        "def read_view_indexes_per_point(prefix_seq, visible_view_indexes, point_cloud_count):\n",
        "    # Read the view indexes per point into a 2-dimension binary matrix\n",
        "    view_indexes_per_point = np.zeros((point_cloud_count, len(visible_view_indexes)))\n",
        "    point_count = -1\n",
        "    with open(str(prefix_seq / 'view_indexes_per_point')) as fp:\n",
        "        for line in fp:\n",
        "            if int(line) < 0:\n",
        "                point_count = point_count + 1\n",
        "            else:\n",
        "                view_indexes_per_point[point_count][visible_view_indexes.index(int(line))] = 1\n",
        "    return view_indexes_per_point\n",
        "\n",
        "\n",
        "def read_pose_data(prefix_seq):\n",
        "    stream = open(str(prefix_seq / \"motion.yaml\"), 'r')\n",
        "    doc = yaml.load(stream)\n",
        "    keys, values = doc.items()\n",
        "    poses = values[1]\n",
        "    return poses"
      ],
      "metadata": {
        "id": "Lx9WmzTlCC_L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_scale_estimation(extrinsics, point_cloud):\n",
        "    max_bound = np.zeros((3,), dtype=np.float32)\n",
        "    min_bound = np.zeros((3,), dtype=np.float32)\n",
        "\n",
        "    for i, extrinsic in enumerate(extrinsics):\n",
        "        if i == 0:\n",
        "            max_bound = extrinsic[:3, 3]\n",
        "            min_bound = extrinsic[:3, 3]\n",
        "        else:\n",
        "            temp = extrinsic[:3, 3]\n",
        "            max_bound = np.maximum(max_bound, temp)\n",
        "            min_bound = np.minimum(min_bound, temp)\n",
        "\n",
        "    norm_1 = np.linalg.norm(max_bound - min_bound, ord=2)\n",
        "\n",
        "    max_bound = np.zeros((3,), dtype=np.float32)\n",
        "    min_bound = np.zeros((3,), dtype=np.float32)\n",
        "    for i, point in enumerate(point_cloud):\n",
        "        if i == 0:\n",
        "            max_bound = np.asarray(point[:3], dtype=np.float32)\n",
        "            min_bound = np.asarray(point[:3], dtype=np.float32)\n",
        "        else:\n",
        "            temp = np.asarray(point[:3], dtype=np.float32)\n",
        "            if np.any(np.isnan(temp)):\n",
        "                continue\n",
        "            max_bound = np.maximum(max_bound, temp)\n",
        "            min_bound = np.minimum(min_bound, temp)\n",
        "\n",
        "    norm_2 = np.linalg.norm(max_bound - min_bound, ord=2)\n",
        "\n",
        "    return max(1.0, max(norm_1, norm_2))\n",
        "\n",
        "\n",
        "def get_extrinsic_matrix_and_projection_matrix(poses, intrinsic_matrix, visible_view_count):\n",
        "    projection_matrices = []\n",
        "    extrinsic_matrices = []\n",
        "    for i in range(visible_view_count):\n",
        "        rigid_transform = quaternion_matrix(\n",
        "            [poses[\"poses[\" + str(i) + \"]\"]['orientation']['w'], poses[\"poses[\" + str(i) + \"]\"]['orientation']['x'],\n",
        "             poses[\"poses[\" + str(i) + \"]\"]['orientation']['y'],\n",
        "             poses[\"poses[\" + str(i) + \"]\"]['orientation']['z']])\n",
        "        rigid_transform[0][3] = poses[\"poses[\" + str(i) + \"]\"]['position']['x']\n",
        "        rigid_transform[1][3] = poses[\"poses[\" + str(i) + \"]\"]['position']['y']\n",
        "        rigid_transform[2][3] = poses[\"poses[\" + str(i) + \"]\"]['position']['z']\n",
        "\n",
        "        transform = np.asmatrix(rigid_transform)\n",
        "        transform = np.linalg.inv(transform)\n",
        "\n",
        "        extrinsic_matrices.append(transform)\n",
        "        projection_matrices.append(np.dot(intrinsic_matrix, transform))\n",
        "\n",
        "    return extrinsic_matrices, projection_matrices\n",
        "\n",
        "\n",
        "def get_color_imgs(prefix_seq, visible_view_indexes, start_h, end_h, start_w, end_w, downsampling_factor, is_hsv=False):\n",
        "    imgs = []\n",
        "    for i in visible_view_indexes:\n",
        "        img = cv2.imread(str(prefix_seq / \"{:08d}.jpg\".format(i)))\n",
        "        downsampled_img = cv2.resize(img, (0, 0), fx=1. / downsampling_factor, fy=1. / downsampling_factor)\n",
        "        cropped_downsampled_img = downsampled_img[start_h:end_h, start_w:end_w, :]\n",
        "        if is_hsv:\n",
        "            cropped_downsampled_img = cv2.cvtColor(cropped_downsampled_img, cv2.COLOR_BGR2HSV_FULL)\n",
        "        imgs.append(cropped_downsampled_img)\n",
        "    height, width, channel = imgs[0].shape\n",
        "    imgs = np.array(imgs, dtype=\"float32\")\n",
        "    imgs = np.reshape(imgs, (-1, height, width, channel))\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def compute_sanity_threshold(sanity_array, inlier_percentage):\n",
        "    # Use histogram to cluster into different contaminated levels\n",
        "    hist, bin_edges = np.histogram(sanity_array, bins=np.arange(1000) * np.max(sanity_array) / 1000.0,\n",
        "                                   density=True)\n",
        "    histogram_percentage = hist * np.diff(bin_edges)\n",
        "    percentage = inlier_percentage\n",
        "    # Let's assume there are a certain percent of points in each frame that are not contaminated\n",
        "    # Get sanity threshold from counting histogram bins\n",
        "    max_index = np.argmax(histogram_percentage)\n",
        "    histogram_sum = histogram_percentage[max_index]\n",
        "    pos_counter = 1\n",
        "    neg_counter = 1\n",
        "    # Assume the sanity value is a one-peak distribution\n",
        "    while True:\n",
        "        if max_index + pos_counter < len(histogram_percentage):\n",
        "            histogram_sum = histogram_sum + histogram_percentage[max_index + pos_counter]\n",
        "            pos_counter = pos_counter + 1\n",
        "            if histogram_sum >= percentage:\n",
        "                sanity_threshold_max = bin_edges[max_index + pos_counter]\n",
        "                sanity_threshold_min = bin_edges[max_index - neg_counter + 1]\n",
        "                break\n",
        "\n",
        "        if max_index - neg_counter >= 0:\n",
        "            histogram_sum = histogram_sum + histogram_percentage[max_index - neg_counter]\n",
        "            neg_counter = neg_counter + 1\n",
        "            if histogram_sum >= percentage:\n",
        "                sanity_threshold_max = bin_edges[max_index + pos_counter]\n",
        "                sanity_threshold_min = bin_edges[max_index - neg_counter + 1]\n",
        "                break\n",
        "\n",
        "        if max_index + pos_counter >= len(histogram_percentage) and max_index - neg_counter < 0:\n",
        "            sanity_threshold_max = np.max(bin_edges)\n",
        "            sanity_threshold_min = np.min(bin_edges)\n",
        "            break\n",
        "    return sanity_threshold_min, sanity_threshold_max\n",
        "\n",
        "\n",
        "def get_clean_point_list(imgs, point_cloud, view_indexes_per_point, mask_boundary, inlier_percentage,\n",
        "                         projection_matrices,\n",
        "                         extrinsic_matrices, is_hsv):\n",
        "    array_3D_points = np.asarray(point_cloud).reshape((-1, 4))\n",
        "    if inlier_percentage <= 0.0 or inlier_percentage >= 1.0:\n",
        "        return list()\n",
        "\n",
        "    point_cloud_contamination_accumulator = np.zeros(array_3D_points.shape[0], dtype=np.int32)\n",
        "    point_cloud_appearance_count = np.zeros(array_3D_points.shape[0], dtype=np.int32)\n",
        "    height, width, channel = imgs[0].shape\n",
        "    valid_frame_count = 0\n",
        "    mask_boundary = mask_boundary.reshape((-1, 1))\n",
        "    for i in range(len(projection_matrices)):\n",
        "        img = imgs[i]\n",
        "        projection_matrix = projection_matrices[i]\n",
        "        extrinsic_matrix = extrinsic_matrices[i]\n",
        "        img = np.array(img, dtype=np.float32) / 255.0\n",
        "        # imgs might be in HSV or BGR colorspace depending on the settings beyond this function\n",
        "        if not is_hsv:\n",
        "            img_filtered = cv2.bilateralFilter(src=img, d=7, sigmaColor=25, sigmaSpace=25)\n",
        "            img_hsv = cv2.cvtColor(img_filtered, cv2.COLOR_BGR2HSV_FULL)\n",
        "        else:\n",
        "            img_bgr = cv2.cvtColor(img, cv2.COLOR_HSV2BGR_FULL)\n",
        "            img_filtered = cv2.bilateralFilter(src=img_bgr, d=7, sigmaColor=25, sigmaSpace=25)\n",
        "            img_hsv = cv2.cvtColor(img_filtered, cv2.COLOR_BGR2HSV_FULL)\n",
        "\n",
        "        view_indexes_frame = np.asarray(view_indexes_per_point[:, i]).reshape((-1))\n",
        "        visible_point_indexes = np.where(view_indexes_frame > 0.5)\n",
        "        visible_point_indexes = visible_point_indexes[0]\n",
        "        points_3D_camera = np.einsum('ij,mj->mi', extrinsic_matrix, array_3D_points)\n",
        "        points_3D_camera = points_3D_camera / points_3D_camera[:, 3].reshape((-1, 1))\n",
        "\n",
        "        points_2D_image = np.einsum('ij,mj->mi', projection_matrix, array_3D_points)\n",
        "        points_2D_image = points_2D_image / points_2D_image[:, 2].reshape((-1, 1))\n",
        "\n",
        "        visible_points_2D_image = points_2D_image[visible_point_indexes, :].reshape((-1, 3))\n",
        "        visible_points_3D_camera = points_3D_camera[visible_point_indexes, :].reshape((-1, 4))\n",
        "        indexes = np.where((visible_points_2D_image[:, 0] <= width - 1) & (visible_points_2D_image[:, 0] >= 0) &\n",
        "                           (visible_points_2D_image[:, 1] <= height - 1) & (visible_points_2D_image[:, 1] >= 0)\n",
        "                           & (visible_points_3D_camera[:, 2] > 0))\n",
        "        indexes = indexes[0]\n",
        "        in_image_point_1D_locations = (np.round(visible_points_2D_image[indexes, 0]) +\n",
        "                                       np.round(visible_points_2D_image[indexes, 1]) * width).astype(\n",
        "            np.int32).reshape((-1))\n",
        "        temp_mask = mask_boundary[in_image_point_1D_locations, :]\n",
        "        indexes_2 = np.where(temp_mask[:, 0] == 255)\n",
        "        indexes_2 = indexes_2[0]\n",
        "        in_mask_point_1D_locations = in_image_point_1D_locations[indexes_2]\n",
        "        points_depth = visible_points_3D_camera[indexes[indexes_2], 2]\n",
        "        img_hsv = img_hsv.reshape((-1, 3))\n",
        "        points_brightness = img_hsv[in_mask_point_1D_locations, 2]\n",
        "        sanity_array = points_depth ** 2 * points_brightness\n",
        "        point_cloud_appearance_count[visible_point_indexes[indexes[indexes_2]]] += 1\n",
        "        if sanity_array.shape[0] < 2:\n",
        "            continue\n",
        "        valid_frame_count += 1\n",
        "        sanity_threshold_min, sanity_threshold_max = compute_sanity_threshold(sanity_array, inlier_percentage)\n",
        "        indexes_3 = np.where((sanity_array <= sanity_threshold_min) | (sanity_array >= sanity_threshold_max))\n",
        "        indexes_3 = indexes_3[0]\n",
        "        point_cloud_contamination_accumulator[visible_point_indexes[indexes[indexes_2[indexes_3]]]] += 1\n",
        "\n",
        "    clean_point_cloud_array = (point_cloud_contamination_accumulator < point_cloud_appearance_count / 2).astype(\n",
        "        np.float32)\n",
        "    print(\"{} points eliminated\".format(int(clean_point_cloud_array.shape[0] - np.sum(clean_point_cloud_array))))\n",
        "    return clean_point_cloud_array\n",
        "\n",
        "\n",
        "def get_visible_count_per_point(view_indexes_per_point):\n",
        "    appearing_count = np.reshape(np.sum(view_indexes_per_point, axis=-1), (-1, 1))\n",
        "    return appearing_count\n",
        "\n",
        "\n",
        "def generating_pos_and_increment(idx, visible_view_indexes, adjacent_range):\n",
        "    # We use the remainder of the overall idx to retrieve the visible view\n",
        "    visible_view_idx = idx % len(visible_view_indexes)\n",
        "\n",
        "    adjacent_range_list = []\n",
        "    adjacent_range_list.append(adjacent_range[0])\n",
        "    adjacent_range_list.append(adjacent_range[1])\n",
        "\n",
        "    if len(visible_view_indexes) <= 2 * adjacent_range_list[0]:\n",
        "        adjacent_range_list[0] = len(visible_view_indexes) // 2\n",
        "\n",
        "    if visible_view_idx <= adjacent_range_list[0] - 1:\n",
        "        increment = random.randint(adjacent_range_list[0],\n",
        "                                   min(adjacent_range_list[1], len(visible_view_indexes) - 1 - visible_view_idx))\n",
        "    elif visible_view_idx >= len(visible_view_indexes) - adjacent_range_list[0]:\n",
        "        increment = -random.randint(adjacent_range_list[0], min(adjacent_range_list[1], visible_view_idx))\n",
        "\n",
        "    else:\n",
        "        # which direction should we increment\n",
        "        direction = random.randint(0, 1)\n",
        "        if direction == 1:\n",
        "            increment = random.randint(adjacent_range_list[0],\n",
        "                                       min(adjacent_range_list[1], len(visible_view_indexes) - 1 - visible_view_idx))\n",
        "        else:\n",
        "            increment = -random.randint(adjacent_range_list[0], min(adjacent_range_list[1], visible_view_idx))\n",
        "\n",
        "    return [visible_view_idx, increment]\n",
        "\n",
        "\n",
        "def get_pair_color_imgs(prefix_seq, pair_indexes, start_h, end_h, start_w, end_w, downsampling_factor, is_hsv,\n",
        "                        rgb_mode):\n",
        "    imgs = []\n",
        "    for i in pair_indexes:\n",
        "        img = cv2.imread(str(Path(prefix_seq) / \"{:08d}.jpg\".format(i)))\n",
        "        downsampled_img = cv2.resize(img, (0, 0), fx=1. / downsampling_factor, fy=1. / downsampling_factor)\n",
        "        downsampled_img = downsampled_img[start_h:end_h, start_w:end_w, :]\n",
        "        if is_hsv:\n",
        "            downsampled_img = cv2.cvtColor(downsampled_img, cv2.COLOR_BGR2HSV_FULL)\n",
        "        else:\n",
        "            if rgb_mode == \"rgb\":\n",
        "                downsampled_img = cv2.cvtColor(downsampled_img, cv2.COLOR_BGR2RGB)\n",
        "        imgs.append(downsampled_img)\n",
        "    height, width, channel = imgs[0].shape\n",
        "    imgs = np.asarray(imgs, dtype=np.uint8)\n",
        "    imgs = imgs.reshape((-1, height, width, channel))\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def get_torch_training_data(pair_extrinsics, pair_projections, pair_indexes, point_cloud, mask_boundary,\n",
        "                            view_indexes_per_point, clean_point_list, visible_view_indexes):\n",
        "    height = mask_boundary.shape[0]\n",
        "    width = mask_boundary.shape[1]\n",
        "    pair_depth_mask_imgs = []\n",
        "    pair_depth_imgs = []\n",
        "\n",
        "    pair_flow_imgs = []\n",
        "    flow_image_1 = np.zeros((height, width, 2), dtype=np.float32)\n",
        "    flow_image_2 = np.zeros((height, width, 2), dtype=np.float32)\n",
        "\n",
        "    pair_flow_mask_imgs = []\n",
        "    flow_mask_image_1 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "    flow_mask_image_2 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "\n",
        "    # We only use inlier points\n",
        "    array_3D_points = np.asarray(point_cloud).reshape((-1, 4))\n",
        "    for i in range(2):\n",
        "        projection_matrix = pair_projections[i]\n",
        "        extrinsic_matrix = pair_extrinsics[i]\n",
        "\n",
        "        if i == 0:\n",
        "            points_2D_image_1 = np.einsum('ij,mj->mi', projection_matrix, array_3D_points)\n",
        "            points_2D_image_1 = np.round(points_2D_image_1 / points_2D_image_1[:, 2].reshape((-1, 1)))\n",
        "            points_3D_camera_1 = np.einsum('ij,mj->mi', extrinsic_matrix, array_3D_points)\n",
        "            points_3D_camera_1 = points_3D_camera_1 / points_3D_camera_1[:, 3].reshape((-1, 1))\n",
        "        else:\n",
        "            points_2D_image_2 = np.einsum('ij,mj->mi', projection_matrix, array_3D_points)\n",
        "            points_2D_image_2 = np.round(points_2D_image_2 / points_2D_image_2[:, 2].reshape((-1, 1)))\n",
        "            points_3D_camera_2 = np.einsum('ij,mj->mi', extrinsic_matrix, array_3D_points)\n",
        "            points_3D_camera_2 = points_3D_camera_2 / points_3D_camera_2[:, 3].reshape((-1, 1))\n",
        "\n",
        "    mask_boundary = mask_boundary.reshape((-1, 1))\n",
        "    flow_image_1 = flow_image_1.reshape((-1, 2))\n",
        "    flow_image_2 = flow_image_2.reshape((-1, 2))\n",
        "    flow_mask_image_1 = flow_mask_image_1.reshape((-1, 1))\n",
        "    flow_mask_image_2 = flow_mask_image_2.reshape((-1, 1))\n",
        "\n",
        "    points_2D_image_1 = points_2D_image_1.reshape((-1, 3))\n",
        "    points_2D_image_2 = points_2D_image_2.reshape((-1, 3))\n",
        "    points_3D_camera_1 = points_3D_camera_1.reshape((-1, 4))\n",
        "    points_3D_camera_2 = points_3D_camera_2.reshape((-1, 4))\n",
        "\n",
        "    point_visibility_1 = np.asarray(view_indexes_per_point[:, visible_view_indexes.index(pair_indexes[0])]).reshape(\n",
        "        (-1))\n",
        "    if len(clean_point_list) != 0:\n",
        "        visible_point_indexes_1 = np.where((point_visibility_1 > 0.5) & (clean_point_list > 0.5))\n",
        "    else:\n",
        "        visible_point_indexes_1 = np.where((point_visibility_1 > 0.5))\n",
        "    visible_point_indexes_1 = visible_point_indexes_1[0]\n",
        "    point_visibility_2 = np.asarray(view_indexes_per_point[:, visible_view_indexes.index(pair_indexes[1])]).reshape(\n",
        "        (-1))\n",
        "\n",
        "    if len(clean_point_list) != 0:\n",
        "        visible_point_indexes_2 = np.where((point_visibility_2 > 0.5) & (clean_point_list > 0.5))\n",
        "    else:\n",
        "        visible_point_indexes_2 = np.where((point_visibility_2 > 0.5))\n",
        "    visible_point_indexes_2 = visible_point_indexes_2[0]\n",
        "    visible_points_3D_camera_1 = points_3D_camera_1[visible_point_indexes_1, :].reshape((-1, 4))\n",
        "    visible_points_2D_image_1 = points_2D_image_1[visible_point_indexes_1, :].reshape((-1, 3))\n",
        "    visible_points_3D_camera_2 = points_3D_camera_2[visible_point_indexes_2, :].reshape((-1, 4))\n",
        "    visible_points_2D_image_2 = points_2D_image_2[visible_point_indexes_2, :].reshape((-1, 3))\n",
        "\n",
        "    in_image_indexes_1 = np.where(\n",
        "        (visible_points_2D_image_1[:, 0] <= width - 1) & (visible_points_2D_image_1[:, 0] >= 0) &\n",
        "        (visible_points_2D_image_1[:, 1] <= height - 1) & (visible_points_2D_image_1[:, 1] >= 0)\n",
        "        & (visible_points_3D_camera_1[:, 2] > 0))\n",
        "    in_image_indexes_1 = in_image_indexes_1[0]\n",
        "    in_image_point_1D_locations_1 = (np.round(visible_points_2D_image_1[in_image_indexes_1, 0]) +\n",
        "                                     np.round(visible_points_2D_image_1[in_image_indexes_1, 1]) * width).astype(\n",
        "        np.int32).reshape((-1))\n",
        "    temp_mask_1 = mask_boundary[in_image_point_1D_locations_1, :]\n",
        "    in_mask_indexes_1 = np.where(temp_mask_1[:, 0] == 255)\n",
        "    in_mask_indexes_1 = in_mask_indexes_1[0]\n",
        "    in_mask_point_1D_locations_1 = in_image_point_1D_locations_1[in_mask_indexes_1]\n",
        "    flow_mask_image_1[in_mask_point_1D_locations_1, 0] = 1.0\n",
        "\n",
        "    in_image_indexes_2 = np.where(\n",
        "        (visible_points_2D_image_2[:, 0] <= width - 1) & (visible_points_2D_image_2[:, 0] >= 0) &\n",
        "        (visible_points_2D_image_2[:, 1] <= height - 1) & (visible_points_2D_image_2[:, 1] >= 0)\n",
        "        & (visible_points_3D_camera_2[:, 2] > 0))\n",
        "    in_image_indexes_2 = in_image_indexes_2[0]\n",
        "    in_image_point_1D_locations_2 = (np.round(visible_points_2D_image_2[in_image_indexes_2, 0]) +\n",
        "                                     np.round(visible_points_2D_image_2[in_image_indexes_2, 1]) * width).astype(\n",
        "        np.int32).reshape((-1))\n",
        "    temp_mask_2 = mask_boundary[in_image_point_1D_locations_2, :]\n",
        "    in_mask_indexes_2 = np.where(temp_mask_2[:, 0] == 255)\n",
        "    in_mask_indexes_2 = in_mask_indexes_2[0]\n",
        "    in_mask_point_1D_locations_2 = in_image_point_1D_locations_2[in_mask_indexes_2]\n",
        "    flow_mask_image_2[in_mask_point_1D_locations_2, 0] = 1.0\n",
        "\n",
        "    flow_image_1[in_mask_point_1D_locations_1, :] = points_2D_image_2[\n",
        "                                                    visible_point_indexes_1[in_image_indexes_1[in_mask_indexes_1]],\n",
        "                                                    :2] - \\\n",
        "                                                    points_2D_image_1[\n",
        "                                                    visible_point_indexes_1[in_image_indexes_1[in_mask_indexes_1]], :2]\n",
        "    flow_image_2[in_mask_point_1D_locations_2, :] = points_2D_image_1[\n",
        "                                                    visible_point_indexes_2[in_image_indexes_2[in_mask_indexes_2]],\n",
        "                                                    :2] - \\\n",
        "                                                    points_2D_image_2[\n",
        "                                                    visible_point_indexes_2[in_image_indexes_2[in_mask_indexes_2]], :2]\n",
        "\n",
        "    flow_image_1[:, 0] /= width\n",
        "    flow_image_1[:, 1] /= height\n",
        "    flow_image_2[:, 0] /= width\n",
        "    flow_image_2[:, 1] /= height\n",
        "\n",
        "    outlier_indexes_1 = np.where((np.abs(flow_image_1[:, 0]) > 5.0) | (np.abs(flow_image_1[:, 1]) > 5.0))[0]\n",
        "    outlier_indexes_2 = np.where((np.abs(flow_image_2[:, 0]) > 5.0) | (np.abs(flow_image_2[:, 1]) > 5.0))[0]\n",
        "    flow_mask_image_1[outlier_indexes_1, 0] = 0.0\n",
        "    flow_mask_image_2[outlier_indexes_2, 0] = 0.0\n",
        "    flow_image_1[outlier_indexes_1, 0] = 0.0\n",
        "    flow_image_2[outlier_indexes_2, 0] = 0.0\n",
        "    flow_image_1[outlier_indexes_1, 1] = 0.0\n",
        "    flow_image_2[outlier_indexes_2, 1] = 0.0\n",
        "\n",
        "    depth_img_1 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "    depth_img_2 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "    depth_mask_img_1 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "    depth_mask_img_2 = np.zeros((height, width, 1), dtype=np.float32)\n",
        "    depth_img_1 = depth_img_1.reshape((-1, 1))\n",
        "    depth_img_2 = depth_img_2.reshape((-1, 1))\n",
        "    depth_mask_img_1 = depth_mask_img_1.reshape((-1, 1))\n",
        "    depth_mask_img_2 = depth_mask_img_2.reshape((-1, 1))\n",
        "\n",
        "    depth_img_1[in_mask_point_1D_locations_1, 0] = points_3D_camera_1[\n",
        "        visible_point_indexes_1[in_image_indexes_1[in_mask_indexes_1]], 2]\n",
        "    depth_img_2[in_mask_point_1D_locations_2, 0] = points_3D_camera_2[\n",
        "        visible_point_indexes_2[in_image_indexes_2[in_mask_indexes_2]], 2]\n",
        "    depth_mask_img_1[in_mask_point_1D_locations_1, 0] = 1.0\n",
        "    depth_mask_img_2[in_mask_point_1D_locations_2, 0] = 1.0\n",
        "\n",
        "    pair_flow_imgs.append(flow_image_1)\n",
        "    pair_flow_imgs.append(flow_image_2)\n",
        "    pair_flow_imgs = np.array(pair_flow_imgs, dtype=\"float32\")\n",
        "    pair_flow_imgs = np.reshape(pair_flow_imgs, (-1, height, width, 2))\n",
        "\n",
        "    pair_flow_mask_imgs.append(flow_mask_image_1)\n",
        "    pair_flow_mask_imgs.append(flow_mask_image_2)\n",
        "    pair_flow_mask_imgs = np.array(pair_flow_mask_imgs, dtype=\"float32\")\n",
        "    pair_flow_mask_imgs = np.reshape(pair_flow_mask_imgs, (-1, height, width, 1))\n",
        "\n",
        "    pair_depth_mask_imgs.append(depth_mask_img_1)\n",
        "    pair_depth_mask_imgs.append(depth_mask_img_2)\n",
        "    pair_depth_mask_imgs = np.array(pair_depth_mask_imgs, dtype=\"float32\")\n",
        "    pair_depth_mask_imgs = np.reshape(pair_depth_mask_imgs, (-1, height, width, 1))\n",
        "\n",
        "    pair_depth_imgs.append(depth_img_1)\n",
        "    pair_depth_imgs.append(depth_img_2)\n",
        "    pair_depth_imgs = np.array(pair_depth_imgs, dtype=\"float32\")\n",
        "    pair_depth_imgs = np.reshape(pair_depth_imgs, (-1, height, width, 1))\n",
        "\n",
        "    return pair_depth_mask_imgs, pair_depth_imgs, pair_flow_mask_imgs, pair_flow_imgs\n",
        "\n",
        "\n",
        "def init_fn(worker_id):\n",
        "    np.random.seed(10086 + worker_id)\n",
        "\n",
        "\n",
        "def init_net(net, type=\"kaiming\", mode=\"fan_in\", activation_mode=\"relu\", distribution=\"normal\"):\n",
        "    assert (torch.cuda.is_available())\n",
        "    net = net.cuda()\n",
        "    if type == \"glorot\":\n",
        "        glorot_weight_zero_bias(net, distribution=distribution)\n",
        "    else:\n",
        "        kaiming_weight_zero_bias(net, mode=mode, activation_mode=activation_mode, distribution=distribution)\n",
        "    return net"
      ],
      "metadata": {
        "id": "Ksno7dBOC7mr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def glorot_weight_zero_bias(model, distribution=\"uniform\"):\n",
        "    \"\"\"\n",
        "    Initalize parameters of all modules\n",
        "    by initializing weights with glorot  uniform/xavier initialization,\n",
        "    and setting biases to zero.\n",
        "    Weights from batch norm layers are set to 1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: Module\n",
        "    distribution: string\n",
        "    \"\"\"\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, 'weight'):\n",
        "            if not ('BatchNorm' in module.__class__.__name__):\n",
        "                if distribution == \"uniform\":\n",
        "                    torch.nn.init.xavier_uniform_(module.weight, gain=1)\n",
        "                else:\n",
        "                    torch.nn.init.xavier_normal_(module.weight, gain=1)\n",
        "            else:\n",
        "                torch.nn.init.constant_(module.weight, 1)\n",
        "        if hasattr(module, 'bias'):\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "def kaiming_weight_zero_bias(model, mode=\"fan_in\", activation_mode=\"relu\", distribution=\"uniform\"):\n",
        "    if activation_mode == \"leaky_relu\":\n",
        "        print(\"Leaky relu is not supported yet\")\n",
        "        assert False\n",
        "\n",
        "    for module in model.modules():\n",
        "        if hasattr(module, 'weight'):\n",
        "            if not ('BatchNorm' in module.__class__.__name__):\n",
        "                if distribution == \"uniform\":\n",
        "                    torch.nn.init.kaiming_uniform_(module.weight, mode=mode, nonlinearity=activation_mode)\n",
        "                else:\n",
        "                    torch.nn.init.kaiming_normal_(module.weight, mode=mode, nonlinearity=activation_mode)\n",
        "            else:\n",
        "                torch.nn.init.constant_(module.weight, 1)\n",
        "        if hasattr(module, 'bias'):\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "def save_model(model, optimizer, epoch, step, model_path, validation_loss):\n",
        "    torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'step': step,\n",
        "        'validation': validation_loss\n",
        "    }, str(model_path))\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "def visualize_color_image(title, images, rebias=False, is_hsv=False, idx=None):\n",
        "    if idx is None:\n",
        "        for i in range(images.shape[0]):\n",
        "            image = images.data.cpu().numpy()[i]\n",
        "            image = np.moveaxis(image, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            if rebias:\n",
        "                image = image * 0.5 + 0.5\n",
        "            if is_hsv:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR_FULL)\n",
        "            cv2.imshow(title + \"_\" + str(i), image)\n",
        "    else:\n",
        "        for id in idx:\n",
        "            image = images.data.cpu().numpy()[id]\n",
        "            image = np.moveaxis(image, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            if rebias:\n",
        "                image = image * 0.5 + 0.5\n",
        "            if is_hsv:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR_FULL)\n",
        "            cv2.imshow(title + \"_\" + str(id), image)\n",
        "\n",
        "\n",
        "def visualize_depth_map(title, depth_maps, min_value_=None, max_value_=None, idx=None, color_mode=cv2.COLORMAP_JET):\n",
        "    min_value_list = []\n",
        "    max_value_list = []\n",
        "    if idx is None:\n",
        "        for i in range(depth_maps.shape[0]):\n",
        "            depth_map_cpu = depth_maps[i].data.cpu().numpy()\n",
        "\n",
        "            if min_value_ is None and max_value_ is None:\n",
        "                min_value = np.min(depth_map_cpu)\n",
        "                max_value = np.max(depth_map_cpu)\n",
        "                min_value_list.append(min_value)\n",
        "                max_value_list.append(max_value)\n",
        "            else:\n",
        "                min_value = min_value_[i]\n",
        "                max_value = max_value_[i]\n",
        "\n",
        "            depth_map_cpu = np.moveaxis(depth_map_cpu, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            depth_map_visualize = np.abs((depth_map_cpu - min_value) / (max_value - min_value) * 255)\n",
        "            depth_map_visualize[depth_map_visualize > 255] = 255\n",
        "            depth_map_visualize[depth_map_visualize <= 0.0] = 0\n",
        "            depth_map_visualize = cv2.applyColorMap(np.uint8(depth_map_visualize), color_mode)\n",
        "            cv2.imshow(title + \"_\" + str(i), depth_map_visualize)\n",
        "        return min_value_list, max_value_list\n",
        "    else:\n",
        "        for id in idx:\n",
        "            depth_map_cpu = depth_maps[id].data.cpu().numpy()\n",
        "\n",
        "            if min_value_ is None and max_value_ is None:\n",
        "                min_value = np.min(depth_map_cpu)\n",
        "                max_value = np.max(depth_map_cpu)\n",
        "                min_value_list.append(min_value)\n",
        "                max_value_list.append(max_value)\n",
        "            else:\n",
        "                min_value = min_value_[id]\n",
        "                max_value = max_value_[id]\n",
        "\n",
        "            depth_map_cpu = np.moveaxis(depth_map_cpu, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            depth_map_visualize = np.abs((depth_map_cpu - min_value) / (max_value - min_value) * 255)\n",
        "            depth_map_visualize[depth_map_visualize > 255] = 255\n",
        "            depth_map_visualize[depth_map_visualize <= 0.0] = 0\n",
        "            depth_map_visualize = cv2.applyColorMap(np.uint8(depth_map_visualize), color_mode)\n",
        "            cv2.imshow(title + \"_\" + str(id), depth_map_visualize)\n",
        "        return min_value_list, max_value_list\n",
        "\n",
        "\n",
        "def display_depth_map(depth_map, min_value=None, max_value=None, colormode=cv2.COLORMAP_JET):\n",
        "    if min_value is None or max_value is None:\n",
        "        min_value = np.min(depth_map)\n",
        "        max_value = np.max(depth_map)\n",
        "    depth_map_visualize = np.abs((depth_map - min_value) / (max_value - min_value) * 255)\n",
        "    depth_map_visualize[depth_map_visualize > 255] = 255\n",
        "    depth_map_visualize[depth_map_visualize <= 0.0] = 0\n",
        "    depth_map_visualize = cv2.applyColorMap(np.uint8(depth_map_visualize), colormode)\n",
        "    return depth_map_visualize\n",
        "\n",
        "\n",
        "def draw_hsv(flows, title, idx=None):\n",
        "    if idx is None:\n",
        "        flows_cpu = flows.data.cpu().numpy()\n",
        "        for i in range(flows_cpu.shape[0]):\n",
        "            flow = np.moveaxis(flows_cpu[i], [0, 1, 2], [2, 0, 1])\n",
        "            h, w = flow.shape[:2]\n",
        "            fx, fy = flow[:, :, 0] * w, flow[:, :, 1] * h\n",
        "            ang = np.arctan2(fy, fx) + np.pi\n",
        "            v = np.sqrt(fx * fx + fy * fy)\n",
        "            hsv = np.zeros((h, w, 3), np.uint8)\n",
        "            hsv[..., 0] = ang * (180 / np.pi / 2)\n",
        "            hsv[..., 1] = 255\n",
        "            hsv[..., 2] = np.uint8(\n",
        "                np.minimum(v, np.sqrt(0.01 * w * w + 0.01 * h * h)) / np.sqrt(0.01 * w * w + 0.01 * h * h) * 255)\n",
        "            bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "            cv2.imshow(title + str(i), bgr)\n",
        "    else:\n",
        "        flows_cpu = flows.data.cpu().numpy()\n",
        "        for id in idx:\n",
        "            flow = np.moveaxis(flows_cpu[id], [0, 1, 2], [2, 0, 1])\n",
        "            h, w = flow.shape[:2]\n",
        "            fx, fy = flow[:, :, 0] * w, flow[:, :, 1] * h\n",
        "            ang = np.arctan2(fy, fx) + np.pi\n",
        "            v = np.sqrt(fx * fx + fy * fy)\n",
        "            hsv = np.zeros((h, w, 3), np.uint8)\n",
        "            hsv[..., 0] = ang * (180 / np.pi / 2)\n",
        "            hsv[..., 1] = 255\n",
        "            hsv[..., 2] = np.uint8(\n",
        "                np.minimum(v, np.sqrt(0.01 * w * w + 0.01 * h * h)) / np.sqrt(0.01 * w * w + 0.01 * h * h) * 255)\n",
        "            bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "            cv2.imshow(title + str(id), bgr)\n",
        "\n",
        "\n",
        "def write_event(log, step, **data):\n",
        "    data['step'] = step\n",
        "    data['dt'] = datetime.time().isoformat()\n",
        "    log.write(unicode(json.dumps(data, sort_keys=True)))\n",
        "    log.write(unicode('\\n'))\n",
        "    log.flush()\n",
        "\n",
        "\n",
        "def point_cloud_from_depth(depth_map, color_img, mask_img, intrinsic_matrix, point_cloud_downsampling,\n",
        "                           min_threshold=None, max_threshold=None):\n",
        "    point_clouds = []\n",
        "    height, width, channel = color_img.shape\n",
        "\n",
        "    f_x = intrinsic_matrix[0, 0]\n",
        "    c_x = intrinsic_matrix[0, 2]\n",
        "    f_y = intrinsic_matrix[1, 1]\n",
        "    c_y = intrinsic_matrix[1, 2]\n",
        "\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            if h % point_cloud_downsampling == 0 and w % point_cloud_downsampling == 0 and mask_img[h, w] > 0.5:\n",
        "                z = depth_map[h, w]\n",
        "                x = (w - c_x) / f_x * z\n",
        "                y = (h - c_y) / f_y * z\n",
        "                b = color_img[h, w, 0]\n",
        "                g = color_img[h, w, 1]\n",
        "                r = color_img[h, w, 2]\n",
        "                if max_threshold is not None and min_threshold is not None:\n",
        "                    if np.max([r, g, b]) >= max_threshold and np.min([r, g, b]) <= min_threshold:\n",
        "                        point_clouds.append((x, y, z, np.uint8(r), np.uint8(g), np.uint8(b)))\n",
        "                else:\n",
        "                    point_clouds.append((x, y, z, np.uint8(r), np.uint8(g), np.uint8(b)))\n",
        "\n",
        "    point_clouds = np.array(point_clouds, dtype='float32')\n",
        "    point_clouds = np.reshape(point_clouds, (-1, 6))\n",
        "    return point_clouds\n",
        "\n",
        "\n",
        "def write_point_cloud(path, point_cloud):\n",
        "    point_clouds_list = []\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        point_clouds_list.append((point_cloud[i, 0], point_cloud[i, 1], point_cloud[i, 2], point_cloud[i, 3],\n",
        "                                  point_cloud[i, 4], point_cloud[i, 5]))\n",
        "\n",
        "    vertex = np.array(point_clouds_list,\n",
        "                      dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n",
        "    el = PlyElement.describe(vertex, 'vertex')\n",
        "    PlyData([el], text=True).write(path)\n",
        "    return\n",
        "\n",
        "\n",
        "def draw_flow(flows, max_v=None):\n",
        "    batch_size, channel, height, width = flows.shape\n",
        "    flows_x_display = vutils.make_grid(flows[:, 0, :, :].reshape(batch_size, 1, height, width), normalize=False,\n",
        "                                       scale_each=False)\n",
        "    flows_y_display = vutils.make_grid(flows[:, 1, :, :].reshape(batch_size, 1, height, width), normalize=False,\n",
        "                                       scale_each=False)\n",
        "    flows_display = torch.cat([flows_x_display[0, :, :].reshape(1, flows_x_display.shape[1], flows_x_display.shape[2]),\n",
        "                               flows_y_display[0, :, :].reshape(1, flows_x_display.shape[1], flows_x_display.shape[2])],\n",
        "                              dim=0)\n",
        "    flows_display = flows_display.data.cpu().numpy()\n",
        "    flows_display = np.moveaxis(flows_display, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    h, w = flows_display.shape[:2]\n",
        "    fx, fy = flows_display[:, :, 0], flows_display[:, :, 1] * h / w\n",
        "    ang = np.arctan2(fy, fx) + np.pi\n",
        "    v = np.sqrt(fx * fx + fy * fy)\n",
        "    hsv = np.zeros((h, w, 3), np.uint8)\n",
        "    hsv[..., 0] = ang * (180 / np.pi / 2)\n",
        "    hsv[..., 1] = 255\n",
        "    if max_v is None:\n",
        "        hsv[..., 2] = np.uint8(np.minimum(v / np.max(v), 1.0) * 255)\n",
        "    else:\n",
        "        hsv[..., 2] = np.uint8(np.minimum(v / max_v, 1.0) * 255)\n",
        "\n",
        "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR), np.max(v)\n",
        "\n",
        "\n",
        "def stack_and_display(phase, title, step, writer, image_list, return_image=False):\n",
        "    writer.add_image(phase + '/Images/' + title,\n",
        "                     np.moveaxis(np.vstack(image_list), source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "    if return_image:\n",
        "        return np.vstack(image_list)\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "def display_color_sparse_depth_dense_depth_warped_depth_sparse_flow_dense_flow(idx, step, writer, colors_1,\n",
        "                                                                               sparse_depths_1, pred_depths_1,\n",
        "                                                                               warped_depths_2_to_1,\n",
        "                                                                               sparse_flows_1, flows_from_depth_1,\n",
        "                                                                               boundaries,\n",
        "                                                                               phase=\"Training\", is_return_image=False,\n",
        "                                                                               color_reverse=True,\n",
        "                                                                               is_hsv=True, rgb_mode=\"bgr\",\n",
        "                                                                               ):\n",
        "    colors_display = vutils.make_grid((colors_1 * 0.5 + 0.5) * boundaries, normalize=False)\n",
        "    colors_display = np.moveaxis(colors_display.data.cpu().numpy(),\n",
        "                                 source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    if is_hsv:\n",
        "        colors_display = cv2.cvtColor(colors_display, cv2.COLOR_HSV2RGB_FULL)\n",
        "    else:\n",
        "        if rgb_mode == \"bgr\":\n",
        "            colors_display = cv2.cvtColor(colors_display, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    min_depth = torch.min(pred_depths_1)\n",
        "    max_depth = torch.max(pred_depths_1)\n",
        "\n",
        "    pred_depths_display = vutils.make_grid(pred_depths_1, normalize=True, scale_each=False,\n",
        "                                           range=(min_depth.item(), max_depth.item()))\n",
        "    pred_depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(pred_depths_display.data.cpu().numpy(),\n",
        "                                                                       source=[0, 1, 2],\n",
        "                                                                       destination=[2, 0, 1])), cv2.COLORMAP_JET)\n",
        "\n",
        "    sparse_depths_display = vutils.make_grid(sparse_depths_1, normalize=True, scale_each=False,\n",
        "                                             range=(min_depth.item(), max_depth.item()))\n",
        "    sparse_depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(sparse_depths_display.data.cpu().numpy(),\n",
        "                                                                         source=[0, 1, 2],\n",
        "                                                                         destination=[2, 0, 1])), cv2.COLORMAP_JET)\n",
        "\n",
        "    warped_depths_display = vutils.make_grid(warped_depths_2_to_1, normalize=True, scale_each=False,\n",
        "                                             range=(min_depth.item(), max_depth.item()))\n",
        "    warped_depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(warped_depths_display.data.cpu().numpy(),\n",
        "                                                                         source=[0, 1, 2],\n",
        "                                                                         destination=[2, 0, 1])), cv2.COLORMAP_JET)\n",
        "\n",
        "    dense_flows_display, max_v = draw_flow(flows_from_depth_1)\n",
        "    sparse_flows_display, _ = draw_flow(sparse_flows_1, max_v=max_v)\n",
        "\n",
        "    if color_reverse:\n",
        "        pred_depths_display = cv2.cvtColor(pred_depths_display, cv2.COLOR_BGR2RGB)\n",
        "        warped_depths_display = cv2.cvtColor(warped_depths_display, cv2.COLOR_BGR2RGB)\n",
        "        sparse_depths_display = cv2.cvtColor(sparse_depths_display, cv2.COLOR_BGR2RGB)\n",
        "        dense_flows_display = cv2.cvtColor(dense_flows_display, cv2.COLOR_BGR2RGB)\n",
        "        sparse_flows_display = cv2.cvtColor(sparse_flows_display, cv2.COLOR_BGR2RGB)\n",
        "    if is_return_image:\n",
        "        return colors_display, sparse_depths_display.astype(np.float32) / 255.0, pred_depths_display.astype(\n",
        "            np.float32) / 255.0, warped_depths_display.astype(np.float32) / 255.0, sparse_flows_display.astype(\n",
        "            np.float32) / 255.0, dense_flows_display.astype(np.float32) / 255.0\n",
        "    else:\n",
        "        writer.add_image(phase + '/Images/Color_' + str(idx), colors_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Sparse_Depth_' + str(idx), sparse_depths_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Pred_Depth_' + str(idx), pred_depths_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Warped_Depth_' + str(idx), warped_depths_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Sparse_Flow_' + str(idx), sparse_flows_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Dense_Flow_' + str(idx), dense_flows_display, step, dataformats=\"HWC\")\n",
        "        return\n",
        "\n",
        "\n",
        "def display_color_depth_sparse_flow_dense_flow(idx, step, writer, colors_1, pred_depths_1,\n",
        "                                               sparse_flows_1, flows_from_depth_1, is_hsv,\n",
        "                                               phase=\"Training\", is_return_image=False, color_reverse=True\n",
        "                                               ):\n",
        "    colors_display = vutils.make_grid(colors_1 * 0.5 + 0.5, normalize=False)\n",
        "    colors_display = np.moveaxis(colors_display.data.cpu().numpy(),\n",
        "                                 source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    if is_hsv:\n",
        "        colors_display = cv2.cvtColor(colors_display, cv2.COLOR_HSV2RGB_FULL)\n",
        "\n",
        "    pred_depths_display = vutils.make_grid(pred_depths_1, normalize=True, scale_each=True)\n",
        "    pred_depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(pred_depths_display.data.cpu().numpy(),\n",
        "                                                                       source=[0, 1, 2],\n",
        "                                                                       destination=[2, 0, 1])), cv2.COLORMAP_JET)\n",
        "    sparse_flows_display, max_v = draw_flow(sparse_flows_1)\n",
        "    dense_flows_display, _ = draw_flow(flows_from_depth_1, max_v=max_v)\n",
        "    if color_reverse:\n",
        "        pred_depths_display = cv2.cvtColor(pred_depths_display, cv2.COLOR_BGR2RGB)\n",
        "        sparse_flows_display = cv2.cvtColor(sparse_flows_display, cv2.COLOR_BGR2RGB)\n",
        "        dense_flows_display = cv2.cvtColor(dense_flows_display, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if is_return_image:\n",
        "        return colors_display, pred_depths_display.astype(np.float32) / 255.0, \\\n",
        "               sparse_flows_display.astype(np.float32) / 255.0, dense_flows_display.astype(np.float32) / 255.0\n",
        "    else:\n",
        "        writer.add_image(phase + '/Images/Color_' + str(idx), colors_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Pred_Depth_' + str(idx), pred_depths_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Sparse_Flow_' + str(idx), sparse_flows_display, step, dataformats=\"HWC\")\n",
        "        writer.add_image(phase + '/Images/Dense_Flow_' + str(idx), dense_flows_display, step, dataformats=\"HWC\")\n",
        "        return\n",
        "\n",
        "\n",
        "def display_color_pred_depth_sparse_depth(idx, step, writer, colors_1, pred_depth_maps_1, sparse_depth_maps_1,\n",
        "                                          phase, return_image=False):\n",
        "    colors_display = vutils.make_grid(colors_1 * 0.5 + 0.5, normalize=False)\n",
        "    colors_display_hsv = np.moveaxis(colors_display.data.cpu().numpy(),\n",
        "                                     source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    colors_display_hsv[colors_display_hsv < 0.0] = 0.0\n",
        "    colors_display_hsv[colors_display_hsv > 1.0] = 1.0\n",
        "    colors_display_hsv = cv2.cvtColor(colors_display_hsv, cv2.COLOR_HSV2RGB_FULL)\n",
        "\n",
        "    depths_display = vutils.make_grid(pred_depth_maps_1, normalize=True, scale_each=True)\n",
        "    depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(depths_display.data.cpu().numpy(),\n",
        "                                                                  source=[0, 1, 2], destination=[2, 0, 1])),\n",
        "                                       cv2.COLORMAP_JET)\n",
        "\n",
        "    sparse_depths_display = vutils.make_grid(sparse_depth_maps_1, normalize=True, scale_each=True)\n",
        "    sparse_depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(sparse_depths_display.data.cpu().numpy(),\n",
        "                                                                         source=[0, 1, 2], destination=[2, 0, 1])),\n",
        "                                              cv2.COLORMAP_JET)\n",
        "\n",
        "    depths_display = cv2.cvtColor(depths_display, cv2.COLOR_BGR2RGB)\n",
        "    sparse_depths_display = cv2.cvtColor(sparse_depths_display, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if return_image:\n",
        "        return colors_display_hsv, depths_display / 255.0, sparse_depths_display / 255.0\n",
        "    else:\n",
        "        writer.add_image(phase + '/Images/Color_' + str(idx),\n",
        "                         np.moveaxis(colors_display_hsv, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "        writer.add_image(phase + '/Images/Depth_' + str(idx),\n",
        "                         np.moveaxis(depths_display, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "        writer.add_image(phase + '/Images/Sparse_Depth_' + str(idx),\n",
        "                         np.moveaxis(sparse_depths_display, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "        return\n",
        "\n",
        "\n",
        "def display_depth_goal(idx, step, writer, goal_depth_map_1):\n",
        "    depths_display = vutils.make_grid(goal_depth_map_1, normalize=True, scale_each=True)\n",
        "    depths_display = cv2.applyColorMap(np.uint8(255 * np.moveaxis(depths_display.data.cpu().numpy(),\n",
        "                                                                  source=[0, 1, 2], destination=[2, 0, 1])),\n",
        "                                       cv2.COLORMAP_JET)\n",
        "    depths_display = cv2.cvtColor(depths_display, cv2.COLOR_BGR2RGB)\n",
        "    writer.add_image('Training/Images/Goal_Depth_' + str(idx),\n",
        "                     np.moveaxis(depths_display, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "    return depths_display\n",
        "\n",
        "\n",
        "def display_network_weights(depth_estimation_model_student, writer, step):\n",
        "    for name, param in depth_estimation_model_student.named_parameters():\n",
        "        writer.add_histogram(\"Weights/\" + name, param.clone().cpu().data.numpy(), step)\n",
        "\n",
        "\n",
        "def generate_training_output(colors_1, scaled_depth_maps_1, boundaries, intrinsic_matrices, is_hsv, epoch,\n",
        "                             results_root):\n",
        "    color_inputs_cpu = colors_1.data.cpu().numpy()\n",
        "    pred_depths_cpu = scaled_depth_maps_1.data.cpu().numpy()\n",
        "    boundaries_cpu = boundaries.data.cpu().numpy()\n",
        "    intrinsics_cpu = intrinsic_matrices.data.cpu().numpy()\n",
        "    color_imgs = []\n",
        "    pred_depth_imgs = []\n",
        "\n",
        "    for j in range(colors_1.shape[0]):\n",
        "        color_img = color_inputs_cpu[j]\n",
        "        pred_depth_img = pred_depths_cpu[j]\n",
        "\n",
        "        color_img = np.moveaxis(color_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "        color_img = color_img * 0.5 + 0.5\n",
        "        color_img[color_img < 0.0] = 0.0\n",
        "        color_img[color_img > 1.0] = 1.0\n",
        "        color_img = np.uint8(255 * color_img)\n",
        "        if is_hsv:\n",
        "            color_img = cv2.cvtColor(color_img, cv2.COLOR_HSV2BGR_FULL)\n",
        "\n",
        "        pred_depth_img = np.moveaxis(pred_depth_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "\n",
        "        if j == 0:\n",
        "            # Write point cloud\n",
        "            boundary = boundaries_cpu[j]\n",
        "            intrinsic = intrinsics_cpu[j]\n",
        "            boundary = np.moveaxis(boundary, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            point_cloud = point_cloud_from_depth(pred_depth_img, color_img, boundary,\n",
        "                                                 intrinsic,\n",
        "                                                 point_cloud_downsampling=1)\n",
        "            write_point_cloud(\n",
        "                str(results_root / \"point_cloud_epoch_{epoch}_index_{index}.ply\".format(epoch=epoch,\n",
        "                                                                                        index=j)),\n",
        "                point_cloud)\n",
        "\n",
        "        color_img = cv2.resize(color_img, dsize=(300, 300))\n",
        "        pred_depth_img = cv2.resize(pred_depth_img, dsize=(300, 300))\n",
        "        color_imgs.append(color_img)\n",
        "\n",
        "        if j == 0:\n",
        "            histr = cv2.calcHist([pred_depth_img], [0], None, histSize=[100], ranges=[0, 1000])\n",
        "            plt.plot(histr, color='b')\n",
        "            plt.xlim([0, 40])\n",
        "            plt.savefig(\n",
        "                str(results_root / 'generated_depth_hist_{epoch}.jpg'.format(epoch=epoch)))\n",
        "            plt.clf()\n",
        "        display_depth_img = display_depth_map(pred_depth_img)\n",
        "        pred_depth_imgs.append(display_depth_img)\n",
        "\n",
        "    final_color = color_imgs[0]\n",
        "    final_pred_depth = pred_depth_imgs[0]\n",
        "    for j in range(colors_1.shape[0] - 1):\n",
        "        final_color = cv2.hconcat((final_color, color_imgs[j + 1]))\n",
        "        final_pred_depth = cv2.hconcat((final_pred_depth, pred_depth_imgs[j + 1]))\n",
        "\n",
        "    final = cv2.vconcat((final_color, final_pred_depth))\n",
        "    cv2.imwrite(str(results_root / 'generated_mask_{epoch}.jpg'.format(epoch=epoch)),\n",
        "                final)\n",
        "\n",
        "\n",
        "def generate_validation_output(idx, step, writer, colors_1, scaled_depth_maps_1, boundaries, intrinsic_matrices, is_hsv,\n",
        "                               results_root, which_bag):\n",
        "    colors_display = vutils.make_grid(colors_1 * 0.5 + 0.5, normalize=False)\n",
        "    colors_display_hsv = np.moveaxis(colors_display.data.cpu().numpy(),\n",
        "                                     source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    colors_display_hsv[colors_display_hsv < 0.0] = 0.0\n",
        "    colors_display_hsv[colors_display_hsv > 1.0] = 1.0\n",
        "    colors_display_hsv = cv2.cvtColor(colors_display_hsv, cv2.COLOR_HSV2RGB_FULL)\n",
        "    writer.add_image('Validation/Images/Color_' + str(idx),\n",
        "                     np.moveaxis(colors_display_hsv, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "\n",
        "    depths_display = vutils.make_grid(scaled_depth_maps_1, normalize=True, scale_each=True)\n",
        "    depths_display_hsv = cv2.applyColorMap(np.uint8(255 * np.moveaxis(depths_display.data.cpu().numpy(),\n",
        "                                                                      source=[0, 1, 2], destination=[2, 0, 1])),\n",
        "                                           cv2.COLORMAP_JET)\n",
        "    depths_display_hsv = cv2.cvtColor(depths_display_hsv, cv2.COLOR_BGR2RGB)\n",
        "    writer.add_image('Validation/Images/Depth_' + str(idx),\n",
        "                     np.moveaxis(depths_display_hsv, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "\n",
        "    color_inputs_cpu = colors_1.data.cpu().numpy()\n",
        "    pred_depths_cpu = scaled_depth_maps_1.data.cpu().numpy()\n",
        "    boundaries_cpu = boundaries.data.cpu().numpy()\n",
        "    intrinsics_cpu = intrinsic_matrices.data.cpu().numpy()\n",
        "    color_imgs = []\n",
        "    pred_depth_imgs = []\n",
        "\n",
        "    for j in range(colors_1.shape[0]):\n",
        "        color_img = color_inputs_cpu[j]\n",
        "        pred_depth_img = pred_depths_cpu[j]\n",
        "\n",
        "        color_img = np.moveaxis(color_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "        color_img = color_img * 0.5 + 0.5\n",
        "        color_img[color_img < 0.0] = 0.0\n",
        "        color_img[color_img > 1.0] = 1.0\n",
        "        color_img = np.uint8(255 * color_img)\n",
        "        if is_hsv:\n",
        "            color_img = cv2.cvtColor(color_img, cv2.COLOR_HSV2BGR_FULL)\n",
        "\n",
        "        pred_depth_img = np.moveaxis(pred_depth_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "\n",
        "        if j == 0:\n",
        "            # Write point cloud\n",
        "            boundary = boundaries_cpu[j]\n",
        "            intrinsic = intrinsics_cpu[j]\n",
        "            boundary = np.moveaxis(boundary, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            point_cloud = point_cloud_from_depth(pred_depth_img, color_img, boundary,\n",
        "                                                 intrinsic,\n",
        "                                                 point_cloud_downsampling=1)\n",
        "            write_point_cloud(\n",
        "                str(results_root / \"point_cloud_step_{step}_index_{index}_bag_{bag}.ply\".format(step=step,\n",
        "                                                                                                index=j,\n",
        "                                                                                                bag=which_bag)),\n",
        "                point_cloud)\n",
        "\n",
        "        color_imgs.append(color_img)\n",
        "        display_depth_img = display_depth_map(pred_depth_img)\n",
        "        pred_depth_imgs.append(display_depth_img)\n",
        "\n",
        "    final_color = color_imgs[0]\n",
        "    final_pred_depth = pred_depth_imgs[0]\n",
        "    for j in range(colors_1.shape[0] - 1):\n",
        "        final_color = cv2.hconcat((final_color, color_imgs[j + 1]))\n",
        "        final_pred_depth = cv2.hconcat((final_pred_depth, pred_depth_imgs[j + 1]))\n",
        "\n",
        "    final = cv2.vconcat((final_color, final_pred_depth))\n",
        "    cv2.imwrite(str(results_root / 'generated_mask_step_{step}_bag_{bag}.jpg'.format(step=step, bag=which_bag)),\n",
        "                final)\n",
        "    return\n",
        "\n",
        "\n",
        "def generate_test_output(idx, step, writer, colors_1, scaled_depth_maps_1, boundaries, intrinsic_matrices, is_hsv,\n",
        "                         results_root, which_bag, color_mode=cv2.COLORMAP_JET):\n",
        "    colors_display = vutils.make_grid(colors_1 * 0.5 + 0.5, normalize=False)\n",
        "    colors_display_hsv = np.moveaxis(colors_display.data.cpu().numpy(),\n",
        "                                     source=[0, 1, 2], destination=[2, 0, 1])\n",
        "    colors_display_hsv[colors_display_hsv < 0.0] = 0.0\n",
        "    colors_display_hsv[colors_display_hsv > 1.0] = 1.0\n",
        "    colors_display_hsv = cv2.cvtColor(colors_display_hsv, cv2.COLOR_HSV2RGB_FULL)\n",
        "    writer.add_image('Test/Images/Color_' + str(idx),\n",
        "                     np.moveaxis(colors_display_hsv, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "\n",
        "    depths_display = vutils.make_grid(scaled_depth_maps_1, normalize=True, scale_each=True)\n",
        "    depths_display_hsv = cv2.applyColorMap(np.uint8(255 * np.moveaxis(depths_display.data.cpu().numpy(),\n",
        "                                                                      source=[0, 1, 2], destination=[2, 0, 1])),\n",
        "                                           colormap=color_mode)\n",
        "    depths_display_hsv = cv2.cvtColor(depths_display_hsv, cv2.COLOR_BGR2RGB)\n",
        "    writer.add_image('Test/Images/Depth_' + str(idx),\n",
        "                     np.moveaxis(depths_display_hsv, source=[0, 1, 2], destination=[1, 2, 0]), step)\n",
        "\n",
        "    color_inputs_cpu = colors_1.data.cpu().numpy()\n",
        "    pred_depths_cpu = scaled_depth_maps_1.data.cpu().numpy()\n",
        "    boundaries_cpu = boundaries.data.cpu().numpy()\n",
        "    intrinsics_cpu = intrinsic_matrices.data.cpu().numpy()\n",
        "    color_imgs = []\n",
        "    pred_depth_imgs = []\n",
        "\n",
        "    for j in range(colors_1.shape[0]):\n",
        "        color_img = color_inputs_cpu[j]\n",
        "        pred_depth_img = pred_depths_cpu[j]\n",
        "\n",
        "        color_img = np.moveaxis(color_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "        color_img = color_img * 0.5 + 0.5\n",
        "        color_img[color_img < 0.0] = 0.0\n",
        "        color_img[color_img > 1.0] = 1.0\n",
        "        color_img = np.uint8(255 * color_img)\n",
        "        if is_hsv:\n",
        "            color_img = cv2.cvtColor(color_img, cv2.COLOR_HSV2BGR_FULL)\n",
        "\n",
        "        pred_depth_img = np.moveaxis(pred_depth_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "\n",
        "        if j == 0:\n",
        "            # Write point cloud\n",
        "            boundary = boundaries_cpu[j]\n",
        "            intrinsic = intrinsics_cpu[j]\n",
        "            boundary = np.moveaxis(boundary, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "            point_cloud = point_cloud_from_depth(pred_depth_img, color_img, boundary,\n",
        "                                                 intrinsic,\n",
        "                                                 point_cloud_downsampling=1)\n",
        "            write_point_cloud(\n",
        "                str(results_root / \"test_point_cloud_step_{step}_bag_{bag}.ply\".format(step=step, bag=which_bag)),\n",
        "                point_cloud)\n",
        "\n",
        "        color_imgs.append(color_img)\n",
        "        display_depth_img = display_depth_map(pred_depth_img, colormode=color_mode)\n",
        "        pred_depth_imgs.append(display_depth_img)\n",
        "\n",
        "    final_color = color_imgs[0]\n",
        "    final_pred_depth = pred_depth_imgs[0]\n",
        "    for j in range(colors_1.shape[0] - 1):\n",
        "        final_color = cv2.hconcat((final_color, color_imgs[j + 1]))\n",
        "        final_pred_depth = cv2.hconcat((final_pred_depth, pred_depth_imgs[j + 1]))\n",
        "\n",
        "    final = cv2.vconcat((final_color, final_pred_depth))\n",
        "    cv2.imwrite(str(results_root / 'generated_mask_step_{step}_bag_{bag}.jpg'.format(step=step, bag=which_bag)),\n",
        "                final)\n",
        "    return\n",
        "\n",
        "\n",
        "def point_cloud_from_depth_and_initial_pose(depth_map, color_img, mask_img, intrinsic_matrix, translation, rotation,\n",
        "                                            point_cloud_downsampling,\n",
        "                                            min_threshold=None, max_threshold=None):\n",
        "    point_clouds = []\n",
        "    height, width, channel = color_img.shape\n",
        "\n",
        "    f_x = intrinsic_matrix[0, 0]\n",
        "    c_x = intrinsic_matrix[0, 2]\n",
        "    f_y = intrinsic_matrix[1, 1]\n",
        "    c_y = intrinsic_matrix[1, 2]\n",
        "\n",
        "    z_min = -1\n",
        "    z_max = -1\n",
        "\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            if h % point_cloud_downsampling == 0 and w % point_cloud_downsampling == 0 and mask_img[h, w] > 0.5:\n",
        "                z = depth_map[h, w]\n",
        "                if z_min == -1:\n",
        "                    z_min = z\n",
        "                    z_max = z\n",
        "                else:\n",
        "                    z_min = min(z, z_min)\n",
        "                    z_max = max(z, z_max)\n",
        "\n",
        "    scale = 20.0 / (z_max - z_min)\n",
        "\n",
        "    for h in range(height):\n",
        "        for w in range(width):\n",
        "            if h % point_cloud_downsampling == 0 and w % point_cloud_downsampling == 0 and mask_img[h, w] > 0.5:\n",
        "                z = depth_map[h, w]\n",
        "                x = (w - c_x) / f_x * z\n",
        "                y = (h - c_y) / f_y * z\n",
        "                position = np.array([x * scale, y * scale, z * scale], dtype=np.float32).reshape((3, 1))\n",
        "                transformed_position = np.matmul(rotation, position) + translation.reshape((3, 1))\n",
        "\n",
        "                r = color_img[h, w, 2]\n",
        "                g = color_img[h, w, 1]\n",
        "                b = color_img[h, w, 0]\n",
        "                if max_threshold is not None and min_threshold is not None:\n",
        "                    if np.max([r, g, b]) >= max_threshold and np.min([r, g, b]) <= min_threshold:\n",
        "                        point_clouds.append((transformed_position[0], transformed_position[1], transformed_position[2],\n",
        "                                             np.uint8(r), np.uint8(g), np.uint8(b)))\n",
        "                else:\n",
        "                    point_clouds.append((transformed_position[0], transformed_position[1], transformed_position[2],\n",
        "                                         np.uint8(r), np.uint8(g), np.uint8(b)))\n",
        "\n",
        "    point_clouds = np.array(point_clouds, dtype='float32')\n",
        "    point_clouds = np.reshape(point_clouds, (-1, 6))\n",
        "    return point_clouds\n",
        "\n",
        "\n",
        "def read_pose_messages_from_tracker(file_path):\n",
        "    translation_array = []\n",
        "    rotation_array = []\n",
        "    with open(file_path, \"r\") as filestream:\n",
        "        for count, line in enumerate(filestream):\n",
        "            # Skip the header\n",
        "            if count == 0:\n",
        "                continue\n",
        "            array = line.split(\",\")\n",
        "            array = array[5:]\n",
        "            array = np.array(array, dtype=np.float64)\n",
        "            translation_array.append(array[:3])\n",
        "            qx, qy, qz, qw = array[3:]\n",
        "            rotation_matrix = quaternion_matrix([qw, qx, qy, qz])\n",
        "            rotation_array.append(rotation_matrix[:3, :3])\n",
        "    return translation_array, rotation_array\n",
        "\n",
        "\n",
        "def write_test_output_with_initial_pose(results_root, colors_1, scaled_depth_maps_1, boundaries, intrinsic_matrices,\n",
        "                                        is_hsv,\n",
        "                                        image_indexes,\n",
        "                                        translation_dict, rotation_dict, color_mode=cv2.COLORMAP_JET):\n",
        "    color_inputs_cpu = colors_1.data.cpu().numpy()\n",
        "    pred_depths_cpu = (boundaries * scaled_depth_maps_1).data.cpu().numpy()\n",
        "    boundaries_cpu = boundaries.data.cpu().numpy()\n",
        "    intrinsics_cpu = intrinsic_matrices.data.cpu().numpy()\n",
        "\n",
        "    for j in range(colors_1.shape[0]):\n",
        "        print(\"processing {}...\".format(image_indexes[j]))\n",
        "        color_img = color_inputs_cpu[j]\n",
        "        pred_depth_img = pred_depths_cpu[j]\n",
        "\n",
        "        color_img = np.moveaxis(color_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "        color_img = color_img * 0.5 + 0.5\n",
        "        color_img[color_img < 0.0] = 0.0\n",
        "        color_img[color_img > 1.0] = 1.0\n",
        "        color_img = np.uint8(255 * color_img)\n",
        "        if is_hsv:\n",
        "            color_img = cv2.cvtColor(color_img, cv2.COLOR_HSV2BGR_FULL)\n",
        "\n",
        "        pred_depth_img = np.moveaxis(pred_depth_img, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "\n",
        "        # Write point cloud\n",
        "        boundary = boundaries_cpu[j]\n",
        "        intrinsic = intrinsics_cpu[j]\n",
        "        boundary = np.moveaxis(boundary, source=[0, 1, 2], destination=[2, 0, 1])\n",
        "        point_cloud = point_cloud_from_depth_and_initial_pose(pred_depth_img, color_img, boundary, intrinsic,\n",
        "                                                              translation=translation_dict[image_indexes[j]],\n",
        "                                                              rotation=rotation_dict[image_indexes[j]],\n",
        "                                                              point_cloud_downsampling=1,\n",
        "                                                              min_threshold=None, max_threshold=None)\n",
        "\n",
        "        write_point_cloud(str(results_root / (\"test_point_cloud_\" + image_indexes[j] + \".ply\")), point_cloud)\n",
        "        cv2.imwrite(str(results_root / (\"test_color_\" + image_indexes[j] + \".jpg\")), color_img)\n",
        "        display_depth_img = display_depth_map(pred_depth_img, colormode=color_mode)\n",
        "        cv2.imwrite(str(results_root / (\"test_depth_\" + image_indexes[j] + \".jpg\")), display_depth_img)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def quaternion_matrix(quaternion):\n",
        "    \"\"\"Return homogeneous rotation matrix from quaternion.\n",
        "\n",
        "    >>> M = quaternion_matrix([0.99810947, 0.06146124, 0, 0])\n",
        "    >>> numpy.allclose(M, rotation_matrix(0.123, [1, 0, 0]))\n",
        "    True\n",
        "    >>> M = quaternion_matrix([1, 0, 0, 0])\n",
        "    >>> numpy.allclose(M, numpy.identity(4))\n",
        "    True\n",
        "    >>> M = quaternion_matrix([0, 1, 0, 0])\n",
        "    >>> numpy.allclose(M, numpy.diag([1, -1, -1, 1]))\n",
        "    True\n",
        "\n",
        "    \"\"\"\n",
        "    q = np.array(quaternion, dtype=np.float64, copy=True)\n",
        "    n = np.dot(q, q)\n",
        "    if n < np.finfo(float).eps * 4.0:\n",
        "        return np.identity(4)\n",
        "    q *= np.sqrt(2.0 / n)\n",
        "    q = np.outer(q, q)\n",
        "    return np.array([\n",
        "        [1.0 - q[2, 2] - q[3, 3], q[1, 2] - q[3, 0], q[1, 3] + q[2, 0], 0.0],\n",
        "        [q[1, 2] + q[3, 0], 1.0 - q[1, 1] - q[3, 3], q[2, 3] - q[1, 0], 0.0],\n",
        "        [q[1, 3] - q[2, 0], q[2, 3] + q[1, 0], 1.0 - q[1, 1] - q[2, 2], 0.0],\n",
        "        [0.0, 0.0, 0.0, 1.0]])\n",
        "\n",
        "\n",
        "def read_initial_pose_file(file_path):\n",
        "    frame_index_array = []\n",
        "    translation_dict = {}\n",
        "    rotation_dict = {}\n",
        "\n",
        "    with open(file_path, \"r\") as filestream:\n",
        "        for line in filestream:\n",
        "            array = line.split(\", \")\n",
        "            array = np.array(array, dtype=np.float64)\n",
        "            frame_index_array.append(int(array[0]))\n",
        "            translation_dict[\"{:08d}\".format(int(array[0]))] = array[1:4]\n",
        "            rotation_matrix = quaternion_matrix(array[4:])\n",
        "            # flip y and z axes\n",
        "            rotation_matrix[:3, 1] = -rotation_matrix[:3, 1]\n",
        "            rotation_matrix[:3, 2] = -rotation_matrix[:3, 2]\n",
        "            rotation_dict[\"{:08d}\".format(int(array[0]))] = rotation_matrix[:3, :3]\n",
        "    frame_index_array.sort()\n",
        "    return frame_index_array, translation_dict, rotation_dict\n",
        "\n",
        "\n",
        "def get_filenames_from_frame_indexes(sequence_root, frame_index_array):\n",
        "    test_image_list = []\n",
        "    for index in frame_index_array:\n",
        "        temp = list(sequence_root.rglob('{:08d}.jpg'.format(index)))\n",
        "        if len(temp) != 0:\n",
        "            test_image_list.append(temp[0])\n",
        "    test_image_list.sort()\n",
        "    return test_image_list\n",
        "\n",
        "\n",
        "def outlier_detection(i, epoch, sparse_flow_weight, sparse_flow_loss, display, flows_1, flows_from_depth_1,\n",
        "                      flow_masks_1,\n",
        "                      flows_2, flows_from_depth_2, flow_masks_2, folders, boundaries, scaled_depth_maps_1,\n",
        "                      scaled_depth_maps_2, colors_1, colors_2, is_hsv):\n",
        "    print(\"batch {:d} in epoch {:d} has large loss {:.5f}\".format(i, epoch,\n",
        "                                                                  sparse_flow_weight * sparse_flow_loss.item()))\n",
        "\n",
        "    losses_1 = display(\n",
        "        [flows_1, flows_from_depth_1, flow_masks_1])\n",
        "    losses_2 = display(\n",
        "        [flows_2, flows_from_depth_2, flow_masks_2])\n",
        "\n",
        "    indice_1 = torch.argmax(losses_1, dim=0, keepdim=False)\n",
        "    indice_2 = torch.argmax(losses_2, dim=0, keepdim=False)\n",
        "\n",
        "    print(\"pair 1 max loss: {:.5f}, pair 2 max loss: {:.5f}\".format(torch.max(losses_1).item(),\n",
        "                                                                    torch.max(losses_2).item()))\n",
        "    print(folders[indice_1.item()], folders[indice_2.item()])\n",
        "    visualize_color_image(\"mask_sample_\", boundaries, rebias=False, is_hsv=False,\n",
        "                          idx=[indice_1.item(), indice_2.item()])\n",
        "\n",
        "    visualize_color_image(\"original color_1_sample_\", colors_1, rebias=True,\n",
        "                          is_hsv=is_hsv, idx=[indice_1.item()])\n",
        "    visualize_depth_map(\"depth_1_sample_\", scaled_depth_maps_1, idx=[indice_1.item()])\n",
        "    draw_hsv(flows_1, \"sparse_flow_1_sample_\", idx=[indice_1.item()])\n",
        "    draw_hsv(flows_from_depth_1, \"flow from depth_1_sample_\", idx=[indice_1.item()])\n",
        "\n",
        "    visualize_color_image(\"original color_2_sample_\", colors_2, rebias=True,\n",
        "                          is_hsv=is_hsv, idx=[indice_2.item()])\n",
        "    visualize_depth_map(\"depth_2_sample_\", scaled_depth_maps_2, idx=[indice_2.item()])\n",
        "    draw_hsv(flows_2, \"sparse_flow_2_sample_\", idx=[indice_2.item()])\n",
        "    draw_hsv(flows_from_depth_2, \"flow from depth_2_sample_\", idx=[indice_2.item()])\n",
        "    cv2.waitKey()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def outlier_detection_processing(failure_threshold, sparse_masked_l1_loss_detector, flows,\n",
        "                                 flows_from_depth, flow_masks):\n",
        "    failure_detection_loss = sparse_masked_l1_loss_detector(\n",
        "        [flows, flows_from_depth, flow_masks])\n",
        "    indexes = []\n",
        "    for j in range(failure_detection_loss.shape[0]):\n",
        "        if failure_detection_loss[j].item() > failure_threshold:\n",
        "            indexes.append(j)\n",
        "    return indexes, failure_detection_loss\n",
        "\n",
        "\n",
        "def learn_from_teacher(boundaries, colors_1, colors_2, depth_estimation_model_teacher, depth_estimation_model_student,\n",
        "                       scale_invariant_loss):\n",
        "    # Predicted depth from teacher model (where sparse signal can be easily propagated)\n",
        "    goal_depth_maps_1 = depth_estimation_model_teacher(colors_1)\n",
        "    goal_depth_maps_2 = depth_estimation_model_teacher(colors_2)\n",
        "    # Predicted depth from student model\n",
        "    predicted_depth_maps_1 = depth_estimation_model_student(colors_1)\n",
        "    predicted_depth_maps_2 = depth_estimation_model_student(colors_2)\n",
        "\n",
        "    abs_goal_depth_maps_1 = torch.abs(goal_depth_maps_1)\n",
        "    abs_goal_depth_maps_2 = torch.abs(goal_depth_maps_2)\n",
        "\n",
        "    abs_predicted_depth_maps_1 = torch.abs(predicted_depth_maps_1)\n",
        "    abs_predicted_depth_maps_2 = torch.abs(predicted_depth_maps_2)\n",
        "\n",
        "    loss = 0.5 * scale_invariant_loss(\n",
        "        [abs_predicted_depth_maps_1, abs_goal_depth_maps_1, boundaries]) + \\\n",
        "           0.5 * scale_invariant_loss(\n",
        "        [abs_predicted_depth_maps_2, abs_goal_depth_maps_2, boundaries])\n",
        "    return loss, torch.abs(predicted_depth_maps_1), torch.abs(predicted_depth_maps_2), \\\n",
        "           torch.abs(goal_depth_maps_1), torch.abs(goal_depth_maps_2)\n",
        "\n",
        "def save_student_model(model_root, depth_estimation_model_student, optimizer, epoch,\n",
        "                       step, failure_sequences, model_path_student, validation_losses, best_validation_losses,\n",
        "                       save_best_only):\n",
        "    model_path_epoch_student = model_root / 'checkpoint_student_model_epoch_{epoch}.pt'.format(epoch=epoch)\n",
        "    validation_losses = np.array(validation_losses)\n",
        "    best_validation_losses = np.array(best_validation_losses)\n",
        "\n",
        "    # Checkpoint model\n",
        "    save_model(model=depth_estimation_model_student, optimizer=optimizer,\n",
        "               epoch=epoch + 1, step=step,\n",
        "               model_path=model_path_epoch_student, failure_sequences=failure_sequences,\n",
        "               validation_loss=validation_losses)\n",
        "\n",
        "    # Best model\n",
        "    # If we use the validation loss to select our model\n",
        "    if save_best_only:\n",
        "        # Save best validation loss model\n",
        "        if calculate_outlier_robust_validation_loss(validation_losses, best_validation_losses) < 0.0:\n",
        "            print(\"Found better model in terms of validation loss: {:.5f}\".format(np.mean(validation_losses)))\n",
        "            save_model(model=depth_estimation_model_student, optimizer=optimizer,\n",
        "                       epoch=epoch + 1, step=step,\n",
        "                       model_path=model_path_student, failure_sequences=failure_sequences,\n",
        "                       validation_loss=validation_losses)\n",
        "            return validation_losses\n",
        "        else:\n",
        "            return best_validation_losses\n",
        "\n",
        "    else:\n",
        "        save_model(model=depth_estimation_model_student, optimizer=optimizer,\n",
        "                   epoch=epoch + 1, step=step,\n",
        "                   model_path=model_path_student, failure_sequences=failure_sequences,\n",
        "                   validation_loss=validation_losses)\n",
        "        return validation_losses\n",
        "\n",
        "\n",
        "def save_teacher_model(model_root, depth_estimation_model_teacher, optimizer, epoch,\n",
        "                       step, failure_sequences, model_path_teacher, validation_losses, best_validation_losses,\n",
        "                       save_best_only):\n",
        "    model_path_epoch_teacher = model_root / 'checkpoint_teacher_model_epoch_{epoch}.pt'.format(epoch=epoch)\n",
        "    validation_losses = np.array(validation_losses)\n",
        "    best_validation_losses = np.array(best_validation_losses)\n",
        "\n",
        "    # Checkpoint model\n",
        "    save_model(model=depth_estimation_model_teacher, optimizer=optimizer,\n",
        "               epoch=epoch + 1, step=step,\n",
        "               model_path=model_path_epoch_teacher, failure_sequences=failure_sequences,\n",
        "               validation_loss=validation_losses)\n",
        "    # Best model\n",
        "    # If we use the validation loss to select our model\n",
        "    if save_best_only:\n",
        "        # Save best validation loss model\n",
        "        if calculate_outlier_robust_validation_loss(validation_losses, best_validation_losses) < 0.0:\n",
        "            print(\"Found better model in terms of validation loss: {:.5f}\".format(np.mean(validation_losses)))\n",
        "            save_model(model=depth_estimation_model_teacher, optimizer=optimizer,\n",
        "                       epoch=epoch + 1, step=step,\n",
        "                       model_path=model_path_teacher, failure_sequences=failure_sequences,\n",
        "                       validation_loss=validation_losses)\n",
        "            return validation_losses\n",
        "        else:\n",
        "            return best_validation_losses\n",
        "\n",
        "    else:\n",
        "        save_model(model=depth_estimation_model_teacher, optimizer=optimizer,\n",
        "                   epoch=epoch + 1, step=step,\n",
        "                   model_path=model_path_teacher, failure_sequences=failure_sequences,\n",
        "                   validation_loss=validation_losses)\n",
        "        return validation_losses\n",
        "\n",
        "\n",
        "def network_validation(writer, validation_loader, batch_size, epoch, depth_estimation_model_student, device,\n",
        "                       depth_scaling_layer,\n",
        "                       sparse_flow_weight, flow_from_depth_layer, sparse_masked_l1_loss, depth_consistency_weight,\n",
        "                       masked_log_l2_loss,\n",
        "                       is_hsv, depth_warping_layer, results_root, tq, which_bag):\n",
        "    # Validation\n",
        "    # Variable initialization\n",
        "    depth_consistency_loss = torch.tensor(0.0).float().cuda()\n",
        "    sparse_flow_loss = torch.tensor(0.0).float().cuda()\n",
        "    scale_std_loss = torch.tensor(0.0).float().cuda()\n",
        "    validation_losses = []\n",
        "    validation_sparse_flow_losses = []\n",
        "    validation_depth_consistency_losses = []\n",
        "    for param in depth_estimation_model_student.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    sample_batch = np.random.randint(low=0, high=len(validation_loader))\n",
        "    for batch, (\n",
        "            colors_1, colors_2, sparse_depths_1, sparse_depths_2, sparse_depth_masks_1,\n",
        "            sparse_depth_masks_2,\n",
        "            flows_1,\n",
        "            flows_2, flow_masks_1, flow_masks_2, boundaries, rotations,\n",
        "            rotations_inverse, translations, translations_inverse, intrinsic_matrices,\n",
        "            folders) in enumerate(\n",
        "        validation_loader):\n",
        "\n",
        "        colors_1, colors_2, \\\n",
        "        sparse_depths_1, sparse_depths_2, sparse_depth_masks_1, sparse_depth_masks_2, flows_1, flows_2, flow_masks_1, \\\n",
        "        flow_masks_2, \\\n",
        "        boundaries, rotations, rotations_inverse, translations, translations_inverse, intrinsic_matrices = \\\n",
        "            colors_1.to(device), colors_2.to(device), \\\n",
        "            sparse_depths_1.to(device), sparse_depths_2.to(device), \\\n",
        "            sparse_depth_masks_1.to(device), sparse_depth_masks_2.to(device), flows_1.to(\n",
        "                device), flows_2.to(\n",
        "                device), flow_masks_1.to(device), flow_masks_2.to(device), \\\n",
        "            boundaries.to(device), rotations.to(device), \\\n",
        "            rotations_inverse.to(device), translations.to(device), translations_inverse.to(\n",
        "                device), intrinsic_matrices.to(device)\n",
        "\n",
        "        # Binarize the boundaries\n",
        "        boundaries = torch.where(boundaries >= torch.tensor(0.9).float().cuda(),\n",
        "                                 torch.tensor(1.0).float().cuda(), torch.tensor(0.0).float().cuda())\n",
        "        # Remove invalid regions of color images\n",
        "        colors_1 = boundaries * colors_1\n",
        "        colors_2 = boundaries * colors_2\n",
        "\n",
        "        # Predicted depth from student model\n",
        "        predicted_depth_maps_1 = depth_estimation_model_student(colors_1)\n",
        "        predicted_depth_maps_2 = depth_estimation_model_student(colors_2)\n",
        "        scaled_depth_maps_1, normalized_scale_std_1 = depth_scaling_layer(\n",
        "            [torch.abs(predicted_depth_maps_1), sparse_depths_1, sparse_depth_masks_1])\n",
        "        scaled_depth_maps_2, normalized_scale_std_2 = depth_scaling_layer(\n",
        "            [torch.abs(predicted_depth_maps_2), sparse_depths_2, sparse_depth_masks_2])\n",
        "\n",
        "        if sparse_flow_weight > 0.0:\n",
        "            # Sparse optical flow loss\n",
        "            # Optical flow maps calculated using predicted dense depth maps and camera poses\n",
        "            # should agree with the sparse optical flows of feature points from SfM\n",
        "            flows_from_depth_1 = flow_from_depth_layer(\n",
        "                [scaled_depth_maps_1, boundaries, translations, rotations,\n",
        "                 intrinsic_matrices])\n",
        "            flows_from_depth_2 = flow_from_depth_layer(\n",
        "                [scaled_depth_maps_2, boundaries, translations_inverse, rotations_inverse,\n",
        "                 intrinsic_matrices])\n",
        "            flow_masks_1 = flow_masks_1 * boundaries\n",
        "            flow_masks_2 = flow_masks_2 * boundaries\n",
        "            flows_1 = flows_1 * boundaries\n",
        "            flows_2 = flows_2 * boundaries\n",
        "            flows_from_depth_1 = flows_from_depth_1 * boundaries\n",
        "            flows_from_depth_2 = flows_from_depth_2 * boundaries\n",
        "            # If we do not try to detect any failure case from SfM\n",
        "            sparse_flow_loss = 0.5 * sparse_masked_l1_loss(\n",
        "                [flows_1, flows_from_depth_1, flow_masks_1]) + \\\n",
        "                               0.5 * sparse_masked_l1_loss(\n",
        "                [flows_2, flows_from_depth_2, flow_masks_2])\n",
        "\n",
        "        if depth_consistency_weight > 0.0:\n",
        "            # Depth consistency loss\n",
        "            warped_depth_maps_2_to_1, intersect_masks_1 = depth_warping_layer(\n",
        "                [scaled_depth_maps_1, scaled_depth_maps_2, boundaries, translations, rotations,\n",
        "                 intrinsic_matrices])\n",
        "            warped_depth_maps_1_to_2, intersect_masks_2 = depth_warping_layer(\n",
        "                [scaled_depth_maps_2, scaled_depth_maps_1, boundaries, translations_inverse,\n",
        "                 rotations_inverse,\n",
        "                 intrinsic_matrices])\n",
        "            depth_consistency_loss = 0.5 * masked_log_l2_loss(\n",
        "                [scaled_depth_maps_1, warped_depth_maps_2_to_1, intersect_masks_1, translations]) + \\\n",
        "                                     0.5 * masked_log_l2_loss(\n",
        "                [scaled_depth_maps_2, warped_depth_maps_1_to_2, intersect_masks_2, translations])\n",
        "\n",
        "        loss = depth_consistency_weight * depth_consistency_loss + sparse_flow_weight * sparse_flow_loss\n",
        "        if not np.isnan(loss.item()):\n",
        "            validation_losses.append(loss.item())\n",
        "            validation_sparse_flow_losses.append(sparse_flow_weight * sparse_flow_loss.item())\n",
        "            validation_depth_consistency_losses.append(\n",
        "                depth_consistency_weight * depth_consistency_loss.item())\n",
        "            tq.set_postfix(loss='{:.5f} {:.5f}'.format(np.mean(validation_losses), loss.item()),\n",
        "                           loss_depth_consistency='{:.5f} {:.5f}'.format(\n",
        "                               np.mean(validation_depth_consistency_losses),\n",
        "                               depth_consistency_weight * depth_consistency_loss.item()),\n",
        "                           loss_sparse_flow='{:.5f} {:.5f}'.format(np.mean(validation_sparse_flow_losses),\n",
        "                                                                   sparse_flow_weight * sparse_flow_loss.item()))\n",
        "        tq.update(batch_size)\n",
        "\n",
        "        if batch == sample_batch:\n",
        "            generate_validation_output(1, epoch, writer, colors_1, scaled_depth_maps_1 * boundaries, boundaries,\n",
        "                                       intrinsic_matrices,\n",
        "                                       is_hsv, results_root, which_bag)\n",
        "\n",
        "    # TensorboardX\n",
        "    writer.add_scalars('Validation', {'overall': np.mean(validation_losses),\n",
        "                                      'depth consistency': np.mean(validation_depth_consistency_losses),\n",
        "                                      'sparse opt': np.mean(validation_sparse_flow_losses)}, epoch)\n",
        "\n",
        "    return np.mean(validation_losses), validation_losses\n",
        "\n",
        "\n",
        "def calculate_outlier_robust_validation_loss(validation_losses, previous_validation_losses):\n",
        "    if len(validation_losses) == len(previous_validation_losses):\n",
        "        differences = validation_losses - previous_validation_losses\n",
        "\n",
        "        positive = np.sum(np.sum(np.int32(differences > 0.0)) * (differences > 0.0) * differences)\n",
        "        negative = np.sum(np.sum(np.int32(differences < 0.0)) * (differences < 0.0) * differences)\n",
        "        return positive + negative\n",
        "    elif len(validation_losses) > len(previous_validation_losses):\n",
        "        return -1.0\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "\n",
        "def read_pose_corresponding_image_indexes(file_path):\n",
        "    pose_corresponding_video_frame_index_array = []\n",
        "    with open(file_path, \"r\") as filestream:\n",
        "        for pose_index, line in enumerate(filestream):\n",
        "            array = line.split(\", \")\n",
        "            array = np.array(array, dtype=np.float32)\n",
        "            pose_corresponding_video_frame_index_array.append(int(array[0]))\n",
        "    pose_corresponding_video_frame_index_array = np.array(pose_corresponding_video_frame_index_array, dtype=np.float32)\n",
        "    return pose_corresponding_video_frame_index_array\n",
        "\n",
        "\n",
        "def read_pose_corresponding_image_indexes_and_time_difference(file_path):\n",
        "    pose_corresponding_video_frame_index_array = []\n",
        "    pose_corresponding_video_frame_time_difference_array = []\n",
        "    with open(file_path, \"r\") as filestream:\n",
        "        for pose_index, line in enumerate(filestream):\n",
        "            array = line.split(\", \")\n",
        "            array = np.array(array, dtype=np.float32)\n",
        "            pose_corresponding_video_frame_index_array.append(int(array[0]))\n",
        "            pose_corresponding_video_frame_time_difference_array.append(int(array[1]))\n",
        "    pose_corresponding_video_frame_index_array = np.array(pose_corresponding_video_frame_index_array, dtype=np.int32)\n",
        "    pose_corresponding_video_frame_time_difference_array = np.array(\n",
        "        pose_corresponding_video_frame_time_difference_array, dtype=np.int32)\n",
        "    return pose_corresponding_video_frame_index_array, pose_corresponding_video_frame_time_difference_array\n",
        "\n",
        "\n",
        "def synchronize_selected_calibration_poses(root):\n",
        "    pose_messages_path = root / \"poses\"\n",
        "    translation_array_EM, rotation_array_EM = read_pose_messages_from_tracker(str(pose_messages_path))\n",
        "\n",
        "    pose_image_indexes_path = root / \"pose_corresponding_image_indexes\"\n",
        "    pose_corresponding_video_frame_index_array = read_pose_corresponding_image_indexes(str(pose_image_indexes_path))\n",
        "\n",
        "    selected_calibration_image_name_list = list(root.glob('*.jpg'))\n",
        "\n",
        "    # Find the most likely camera position\n",
        "    for calibration_image_name in selected_calibration_image_name_list:\n",
        "        calibration_image_name = str(calibration_image_name)\n",
        "        difference_array = pose_corresponding_video_frame_index_array.astype(np.int32) - int(\n",
        "            calibration_image_name[-12:-4])\n",
        "        # Find if there are some zeros in it\n",
        "        zero_indexes, = np.where(difference_array == 0)\n",
        "\n",
        "        translation = np.zeros((3,), dtype=np.float64)\n",
        "        rotation = np.zeros((3, 3), dtype=np.float64)\n",
        "        # Average over these corresponding EM positions\n",
        "        if zero_indexes.size != 0:\n",
        "            flag = \"\"\n",
        "            sum_count = 0\n",
        "            for count, zero_index in enumerate(zero_indexes):\n",
        "                translation += translation_array_EM[zero_index]\n",
        "                rotation += rotation_array_EM[zero_index]\n",
        "                sum_count = count + 1.0\n",
        "            translation /= sum_count\n",
        "            # print(\"previous\", rotation / sum_count)\n",
        "            if sum_count > 1.0:\n",
        "                rotation = rotation_array_EM[zero_indexes[0]]\n",
        "                # rotation = average_rotation(rotation / sum_count)\n",
        "                # print(\"averaged\", rotation)\n",
        "        # Find the closest EM positions and use that instead\n",
        "        else:\n",
        "            min_indexes = np.argmin(np.abs(difference_array))\n",
        "            flag = \"\"\n",
        "            # If the closest frame are too far away, raise an error for bug inspection\n",
        "            if np.amin(np.abs(difference_array)) > 10:\n",
        "                flag = \"bad\"\n",
        "                print(\"no best matches available for image {}\".format(calibration_image_name))\n",
        "                # raise OSError\n",
        "\n",
        "            if hasattr(min_indexes, \"__len__\"):\n",
        "                # Average over all the corresponding EM positions\n",
        "                sum_count = 0\n",
        "                for count, min_index in enumerate(min_indexes):\n",
        "                    translation += translation_array_EM[min_index]\n",
        "                    rotation += rotation_array_EM[min_index]\n",
        "                    sum_count = count + 1.0\n",
        "                translation /= sum_count\n",
        "                # print(\"previous\", rotation / sum_count)\n",
        "                if sum_count > 1.0:\n",
        "                    rotation = rotation_array_EM[min_indexes[0]]\n",
        "                    # rotation = average_rotation(rotation / sum_count)\n",
        "                    # print(\"averaged\", rotation)\n",
        "            else:\n",
        "                translation = translation_array_EM[min_indexes]\n",
        "                rotation = rotation_array_EM[min_indexes]\n",
        "\n",
        "        with open(calibration_image_name[:-4] + flag + \".coords\", \"w\") as filestream:\n",
        "            for i in range(3):\n",
        "                filestream.write(\"{:.5f},\".format(translation[i]))\n",
        "            for i in range(3):\n",
        "                for j in range(3):\n",
        "                    if i != 2 or j != 2:\n",
        "                        filestream.write(\"{:.5f},\".format(rotation[i][j]))\n",
        "                    else:\n",
        "                        filestream.write(\"{:.5f}\\n\".format(rotation[i][j]))\n",
        "    return\n",
        "\n",
        "\n",
        "def synchronize_image_and_poses(root, tolerance_threshold=1.0e6):\n",
        "    pose_messages_path = root / \"bags\" / \"poses_calibration\"\n",
        "    translation_array_EM, rotation_array_EM = read_pose_messages_from_tracker(str(pose_messages_path))\n",
        "\n",
        "    pose_image_indexes_path = root / \"bags\" / \"pose_corresponding_image_indexes_calibration\"\n",
        "    pose_corresponding_video_frame_index_array, pose_corresponding_video_frame_time_difference_array = \\\n",
        "        read_pose_corresponding_image_indexes_and_time_difference(str(pose_image_indexes_path))\n",
        "\n",
        "    best_matches_pose_indexes = np.where(pose_corresponding_video_frame_time_difference_array < tolerance_threshold)\n",
        "    best_matches_pose_indexes = best_matches_pose_indexes[0]\n",
        "    selected_video_frame_index_array = pose_corresponding_video_frame_index_array[best_matches_pose_indexes]\n",
        "\n",
        "    selected_calibration_root = root / \"selected_calibration_images\"\n",
        "    calibration_root = root / \"calibration_images\"\n",
        "    try:\n",
        "        selected_calibration_root.mkdir(mode=0o777, parents=True)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    for ori_index, selected_video_frame_index in enumerate(selected_video_frame_index_array):\n",
        "\n",
        "        dest = selected_calibration_root / \"{:08d}.jpg\".format(selected_video_frame_index)\n",
        "        if not dest.exists():\n",
        "            shutil.copyfile(str(calibration_root / \"{:08d}.jpg\".format(selected_video_frame_index)),\n",
        "                            str(dest))\n",
        "\n",
        "        translation = translation_array_EM[best_matches_pose_indexes[ori_index]]\n",
        "        rotation = rotation_array_EM[best_matches_pose_indexes[ori_index]]\n",
        "        with open(str(selected_calibration_root / \"{:08d}.coords\".format(selected_video_frame_index)),\n",
        "                  \"w\") as filestream:\n",
        "            for i in range(3):\n",
        "                filestream.write(\"{:.5f},\".format(translation[i]))\n",
        "            for i in range(3):\n",
        "                for j in range(3):\n",
        "                    if i != 2 or j != 2:\n",
        "                        filestream.write(\"{:.5f},\".format(rotation[i][j]))\n",
        "                    else:\n",
        "                        filestream.write(\"{:.5f}\\n\".format(rotation[i][j]))\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def read_camera_to_tcp_transform(root):\n",
        "    transform = np.zeros((3, 4), dtype=np.float)\n",
        "    with open(str(root / \"camera_to_tcp\"), \"r\") as filestream:\n",
        "        for line in filestream:\n",
        "            temp = line.split(\" \")\n",
        "            temp = np.array(temp, dtype=np.float)\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(4):\n",
        "            transform[i, j] = temp[4 * i + j]\n",
        "    return transform[:, :3], transform[:, 3].reshape((3, 1))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    size = 1001\n",
        "    circle = np.zeros((size, size, 3), dtype=np.float32)\n",
        "    circle[:, :, 1] = 255\n",
        "\n",
        "    center = (size - 1) / 2\n",
        "    for y in range(size):\n",
        "        for x in range(size):\n",
        "            fy = (y - center) / size\n",
        "            fx = (x - center) / size\n",
        "            ang = np.arctan2(fy, fx) + np.pi\n",
        "            v = np.sqrt(fx * fx + fy * fy)\n",
        "            circle[y, x, 0] = ang * (180 / np.pi / 2)\n",
        "            circle[y, x, 2] = np.uint8(np.minimum(v, 0.5) * 2.0 * 255)\n",
        "\n",
        "    circle = cv2.cvtColor(np.uint8(circle), cv2.COLOR_HSV2RGB)\n",
        "    plt.figure(figsize=(10,18))\n",
        "    cv2.imwrite(\"/home/xliu89/tmp_ramfs/flow_color_coding.png\", circle)\n",
        "    cv2.waitKey()"
      ],
      "metadata": {
        "id": "NXFNPXbbDBtD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8e29778-12b7-4733-d271-15d7fb6aab76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}