{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1e0c7318",
      "metadata": {
        "id": "1e0c7318"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torchsummary\n",
        "import torch\n",
        "import random\n",
        "from tensorboardX import SummaryWriter\n",
        "import albumentations as albu\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "import models\n",
        "import utils\n",
        "import dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOKYOBFGS5Ki",
        "outputId": "b95679e4-6166-4f05-ed94-887147328c59"
      },
      "id": "UOKYOBFGS5Ki",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "Yn872l56TPFv"
      },
      "id": "Yn872l56TPFv",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "f73ZpppQTYs5"
      },
      "id": "f73ZpppQTYs5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plyfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSTJr4DPTvL7",
        "outputId": "f253cd07-8e69-48a6-a946-63b3a0f269a6"
      },
      "id": "kSTJr4DPTvL7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plyfile\n",
            "  Downloading plyfile-0.9-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.22.4)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f818a826",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "f818a826",
        "outputId": "7e6e543e-6c1b-42c5-8cc6-db9d90ae04fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--input_downsampling INPUT_DOWNSAMPLING]\n",
            "                             --input_size INPUT_SIZE [INPUT_SIZE ...]\n",
            "                             [--selected_frame_index_list SELECTED_FRAME_INDEX_LIST [SELECTED_FRAME_INDEX_LIST ...]]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--num_workers NUM_WORKERS]\n",
            "                             [--num_pre_workers NUM_PRE_WORKERS]\n",
            "                             --adjacent_range ADJACENT_RANGE\n",
            "                             [ADJACENT_RANGE ...] --id_range ID_RANGE\n",
            "                             [ID_RANGE ...]\n",
            "                             [--network_downsampling NETWORK_DOWNSAMPLING]\n",
            "                             [--inlier_percentage INLIER_PERCENTAGE]\n",
            "                             [--testing_patient_id TESTING_PATIENT_ID [TESTING_PATIENT_ID ...]]\n",
            "                             [--load_intermediate_data] [--use_hsv_colorspace]\n",
            "                             [--architecture_summary] [--load_all_frames]\n",
            "                             --trained_model_path TRAINED_MODEL_PATH\n",
            "                             --sequence_root SEQUENCE_ROOT\n",
            "                             --evaluation_result_root EVALUATION_RESULT_ROOT\n",
            "                             --evaluation_data_root EVALUATION_DATA_ROOT\n",
            "                             --phase PHASE\n",
            "                             [--visibility_overlap VISIBILITY_OVERLAP]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --input_size, --adjacent_range, --id_range, --trained_model_path, --sequence_root, --evaluation_result_root, --evaluation_data_root, --phase\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    cv2.destroyAllWindows()\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Self-supervised Depth Estimation on Monocular Endoscopy Dataset -- Evaluate',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('--input_downsampling', type=float, default=4.0,\n",
        "                        help='image downsampling rate')\n",
        "    parser.add_argument('--input_size', nargs='+', type=int, required=True,\n",
        "                        help='input size')\n",
        "    parser.add_argument('--selected_frame_index_list', nargs='+', type=int, required=False, default=None,\n",
        "                        help='selected frame index list)')\n",
        "    parser.add_argument('--batch_size', type=int, default=1, help='batch size for testing')\n",
        "    parser.add_argument('--num_workers', type=int, default=2, help='number of workers for input data loader')\n",
        "    parser.add_argument('--num_pre_workers', type=int, default=8,\n",
        "                        help='number of workers for preprocessing intermediate data')\n",
        "    parser.add_argument('--adjacent_range', nargs='+', type=int, required=True,\n",
        "                        help='interval range for a pair of video frames')\n",
        "    parser.add_argument('--id_range', nargs='+', type=int, required=True,\n",
        "                        help='id range for the training and testing dataset')\n",
        "    parser.add_argument('--network_downsampling', type=int, default=64, help='downsampling of network')\n",
        "    parser.add_argument('--inlier_percentage', type=float, default=0.995,\n",
        "                        help='percentage of inliers of SfM point clouds (for pruning some outliers)')\n",
        "    parser.add_argument('--testing_patient_id', nargs='+', type=int, help='id of the testing patient')\n",
        "    parser.add_argument('--load_intermediate_data', action='store_true', help='whether to load intermediate data')\n",
        "    parser.add_argument('--use_hsv_colorspace', action='store_true',\n",
        "                        help='convert RGB to hsv colorspace')\n",
        "    parser.add_argument('--architecture_summary', action='store_true', help='display the network architecture')\n",
        "    parser.add_argument('--load_all_frames', action='store_true',\n",
        "                        help='whether or not to load all frames in sequence root')\n",
        "    parser.add_argument('--trained_model_path', type=str, required=True, help='path to the trained student model')\n",
        "    parser.add_argument('--sequence_root', type=str, required=True, help='path to the testing sequence')\n",
        "    parser.add_argument('--evaluation_result_root', type=str, required=True,\n",
        "                        help='logging root')\n",
        "    parser.add_argument('--evaluation_data_root', type=str, required=True, help='path to the testing data')\n",
        "    parser.add_argument('--phase', type=str, required=True, help='phase')\n",
        "    parser.add_argument('--visibility_overlap', type=int, default=30, help='overlap of point visibility information')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Fix randomness for reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(10085)\n",
        "    np.random.seed(10085)\n",
        "    random.seed(10085)\n",
        "\n",
        "    # Hyper-parameters\n",
        "    height, width = args.input_size\n",
        "    adjacent_range = args.adjacent_range\n",
        "    id_range = args.id_range\n",
        "    input_downsampling = args.input_downsampling\n",
        "    batch_size = args.batch_size\n",
        "    num_workers = args.num_workers\n",
        "    network_downsampling = args.network_downsampling\n",
        "    inlier_percentage = args.inlier_percentage\n",
        "    testing_patient_id = args.testing_patient_id\n",
        "    load_intermediate_data = args.load_intermediate_data\n",
        "    is_hsv = args.use_hsv_colorspace\n",
        "    display_architecture = args.architecture_summary\n",
        "    selected_frame_index_list = args.selected_frame_index_list\n",
        "    load_all_frames = args.load_all_frames\n",
        "    phase = args.phase\n",
        "    evaluation_result_root = Path(args.evaluation_result_root)\n",
        "    evaluation_data_root = Path(args.evaluation_data_root)\n",
        "    trained_model_path = Path(args.trained_model_path)\n",
        "    sequence_root = Path(args.sequence_root)\n",
        "    visibility_overlap = args.visibility_overlap\n",
        "    num_pre_workers = args.num_pre_workers\n",
        "    currentDT = datetime.datetime.now()\n",
        "\n",
        "    log_root = Path(evaluation_result_root) / \"depth_estimation_evaluation_run_{}_{}_{}_{}_test_id_{}\".format(\n",
        "        currentDT.month,\n",
        "        currentDT.day,\n",
        "        currentDT.hour,\n",
        "        currentDT.minute,\n",
        "        \"_\".join(testing_patient_id))\n",
        "    if not log_root.exists():\n",
        "        log_root.mkdir(parents=True)\n",
        "    writer = SummaryWriter(logdir=str(log_root))\n",
        "    print(\"Tensorboard visualization at {}\".format(str(log_root)))\n",
        "\n",
        "    if selected_frame_index_list is None and not load_all_frames:\n",
        "        raise IOError\n",
        "\n",
        "    # Read all frame indexes\n",
        "    if load_all_frames:\n",
        "        selected_frame_index_list = utils.read_visible_view_indexes(sequence_root)\n",
        "\n",
        "    # Get color image filenames\n",
        "    test_filenames = utils.get_filenames_from_frame_indexes(sequence_root, selected_frame_index_list)\n",
        "    folder_list = utils.get_parent_folder_names(evaluation_data_root, id_range=id_range)\n",
        "\n",
        "    if phase == \"validation\":\n",
        "        test_dataset = dataset.SfMDataset(image_file_names=test_filenames,\n",
        "                                          folder_list=folder_list,\n",
        "                                          adjacent_range=adjacent_range, transform=None,\n",
        "                                          downsampling=input_downsampling,\n",
        "                                          network_downsampling=network_downsampling,\n",
        "                                          inlier_percentage=inlier_percentage,\n",
        "                                          use_store_data=load_intermediate_data,\n",
        "                                          store_data_root=evaluation_data_root,\n",
        "                                          phase=\"validation\", is_hsv=is_hsv,\n",
        "                                          num_pre_workers=num_pre_workers, visible_interval=visibility_overlap,\n",
        "                                          rgb_mode=\"rgb\")\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                                  num_workers=0)\n",
        "        depth_estimation_model = models.FCDenseNet57(n_classes=1)\n",
        "        # Initialize the depth estimation network with Kaiming He initialization\n",
        "        utils.init_net(depth_estimation_model, type=\"kaiming\", mode=\"fan_in\", activation_mode=\"relu\",\n",
        "                       distribution=\"normal\")\n",
        "        # Multi-GPU running\n",
        "        depth_estimation_model = torch.nn.DataParallel(depth_estimation_model)\n",
        "        # Summary network architecture\n",
        "        if display_architecture:\n",
        "            torchsummary.summary(depth_estimation_model, input_size=(3, height, width))\n",
        "\n",
        "        # Load trained model\n",
        "        if trained_model_path.exists():\n",
        "            print(\"Loading {:s} ...\".format(str(trained_model_path)))\n",
        "            state = torch.load(str(trained_model_path))\n",
        "            step = state['step']\n",
        "            epoch = state['epoch']\n",
        "            depth_estimation_model.load_state_dict(state['model'])\n",
        "            print('Restored model, epoch {}, step {}'.format(epoch, step))\n",
        "        else:\n",
        "            print(\"Trained model could not be found\")\n",
        "            raise OSError\n",
        "        depth_estimation_model = depth_estimation_model.module\n",
        "\n",
        "        # Custom layers\n",
        "        depth_scaling_layer = models.DepthScalingLayer()\n",
        "        depth_warping_layer = models.DepthWarpingLayer()\n",
        "        flow_from_depth_layer = models.FlowfromDepthLayer()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Set model to evaluation mode\n",
        "            depth_estimation_model.eval()\n",
        "            # Update progress bar\n",
        "            tq = tqdm.tqdm(total=len(test_loader) * batch_size)\n",
        "            for batch, (\n",
        "                    colors_1, colors_2, sparse_depths_1, sparse_depths_2, sparse_depth_masks_1, sparse_depth_masks_2,\n",
        "                    sparse_flows_1, sparse_flows_2, sparse_flow_masks_1, sparse_flow_masks_2, boundaries,\n",
        "                    rotations_1_wrt_2, rotations_2_wrt_1, translations_1_wrt_2, translations_2_wrt_1, intrinsics,\n",
        "                    folders) in enumerate(test_loader):\n",
        "                colors_1 = colors_1.cuda()\n",
        "                colors_2 = colors_2.cuda()\n",
        "                sparse_depths_1 = sparse_depths_1.cuda()\n",
        "                sparse_depths_2 = sparse_depths_2.cuda()\n",
        "                sparse_depth_masks_1 = sparse_depth_masks_1.cuda()\n",
        "                sparse_depth_masks_2 = sparse_depth_masks_2.cuda()\n",
        "                sparse_flows_1 = sparse_flows_1.cuda()\n",
        "                sparse_flows_2 = sparse_flows_2.cuda()\n",
        "                boundaries = boundaries.cuda()\n",
        "                rotations_1_wrt_2 = rotations_1_wrt_2.cuda()\n",
        "                rotations_2_wrt_1 = rotations_2_wrt_1.cuda()\n",
        "                translations_1_wrt_2 = translations_1_wrt_2.cuda()\n",
        "                translations_2_wrt_1 = translations_2_wrt_1.cuda()\n",
        "                intrinsics = intrinsics.cuda()\n",
        "\n",
        "                tq.update(batch_size)\n",
        "\n",
        "                colors_1 = boundaries * colors_1\n",
        "                colors_2 = boundaries * colors_2\n",
        "                sparse_flows_1 = sparse_flows_1 * boundaries\n",
        "                sparse_flows_2 = sparse_flows_2 * boundaries\n",
        "\n",
        "                predicted_depth_maps_1 = depth_estimation_model(colors_1)\n",
        "                predicted_depth_maps_2 = depth_estimation_model(colors_2)\n",
        "                scaled_depth_maps_1, normalized_scale_std_1 = depth_scaling_layer(\n",
        "                    [torch.abs(predicted_depth_maps_1), sparse_depths_1, sparse_depth_masks_1])\n",
        "                scaled_depth_maps_2, normalized_scale_std_2 = depth_scaling_layer(\n",
        "                    [torch.abs(predicted_depth_maps_2), sparse_depths_2, sparse_depth_masks_2])\n",
        "\n",
        "                depth_array = scaled_depth_maps_1[0].squeeze(dim=0).data.cpu().numpy()\n",
        "                color_array = np.uint8(255 * cv2.cvtColor(\n",
        "                    (colors_1[0].permute(1, 2, 0).data.cpu().numpy() + 1.0) * 0.5, cv2.COLOR_HSV2BGR_FULL))\n",
        "                boundary_array = boundaries[0].squeeze(dim=0).data.cpu().numpy()\n",
        "                intrinsic_array = intrinsics[0].data.cpu().numpy()\n",
        "\n",
        "                # Sparse flow loss\n",
        "                flows_from_depth_1 = flow_from_depth_layer(\n",
        "                    [scaled_depth_maps_1, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                     intrinsics])\n",
        "                flows_from_depth_2 = flow_from_depth_layer(\n",
        "                    [scaled_depth_maps_2, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                     intrinsics])\n",
        "\n",
        "                flows_from_depth_1 = flows_from_depth_1 * boundaries\n",
        "                flows_from_depth_2 = flows_from_depth_2 * boundaries\n",
        "\n",
        "                warped_depth_maps_2_to_1, intersect_masks_1 = depth_warping_layer(\n",
        "                    [scaled_depth_maps_1, scaled_depth_maps_2, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                     intrinsics])\n",
        "                warped_depth_maps_1_to_2, intersect_masks_2 = depth_warping_layer(\n",
        "                    [scaled_depth_maps_2, scaled_depth_maps_1, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                     intrinsics])\n",
        "\n",
        "                colors_1_display, sparse_depths_1_display, pred_depths_1_display, warped_depths_1_display, sparse_flows_1_display, dense_flows_1_display = \\\n",
        "                    utils.display_color_sparse_depth_dense_depth_warped_depth_sparse_flow_dense_flow(idx=1, step=step,\n",
        "                                                                                                     writer=writer,\n",
        "                                                                                                     colors_1=colors_1,\n",
        "                                                                                                     sparse_depths_1=sparse_depths_1,\n",
        "                                                                                                     pred_depths_1=scaled_depth_maps_1 * boundaries,\n",
        "                                                                                                     warped_depths_2_to_1=warped_depth_maps_2_to_1,\n",
        "                                                                                                     sparse_flows_1=sparse_flows_1,\n",
        "                                                                                                     flows_from_depth_1=flows_from_depth_1,\n",
        "                                                                                                     phase=\"validation\",\n",
        "                                                                                                     is_return_image=True,\n",
        "                                                                                                     color_reverse=True,\n",
        "                                                                                                     is_hsv=is_hsv,\n",
        "                                                                                                     rgb_mode=\"rgb\",\n",
        "                                                                                                     boundaries=boundaries\n",
        "                                                                                                     )\n",
        "                colors_2_display, sparse_depths_2_display, pred_depths_2_display, warped_depths_2_display, sparse_flows_2_display, dense_flows_2_display = \\\n",
        "                    utils.display_color_sparse_depth_dense_depth_warped_depth_sparse_flow_dense_flow(idx=2, step=step,\n",
        "                                                                                                     writer=writer,\n",
        "                                                                                                     colors_1=colors_2,\n",
        "                                                                                                     sparse_depths_1=sparse_depths_2,\n",
        "                                                                                                     pred_depths_1=scaled_depth_maps_2 * boundaries,\n",
        "                                                                                                     warped_depths_2_to_1=warped_depth_maps_1_to_2,\n",
        "                                                                                                     sparse_flows_1=sparse_flows_2,\n",
        "                                                                                                     flows_from_depth_1=flows_from_depth_2,\n",
        "                                                                                                     phase=\"validation\",\n",
        "                                                                                                     is_return_image=True,\n",
        "                                                                                                     color_reverse=True,\n",
        "                                                                                                     is_hsv=is_hsv,\n",
        "                                                                                                     rgb_mode=\"rgb\",\n",
        "                                                                                                     boundaries=boundaries\n",
        "                                                                                                     )\n",
        "                image_display = utils.stack_and_display(phase=\"validation\",\n",
        "                                                        title=\"Results (c1, sd1, d1, wd1, sf1, df1, c2, sd2, d2, wd2, sf2, df2)\",\n",
        "                                                        step=step, writer=writer,\n",
        "                                                        image_list=[colors_1_display, sparse_depths_1_display,\n",
        "                                                                    pred_depths_1_display,\n",
        "                                                                    warped_depths_1_display, sparse_flows_1_display,\n",
        "                                                                    dense_flows_1_display,\n",
        "                                                                    colors_2_display, sparse_depths_2_display,\n",
        "                                                                    pred_depths_2_display,\n",
        "                                                                    warped_depths_2_display, sparse_flows_2_display,\n",
        "                                                                    dense_flows_2_display],\n",
        "                                                        return_image=True)\n",
        "                cv2.imwrite(str(log_root / \"{}.png\".format(batch)),\n",
        "                            cv2.cvtColor(np.uint8(image_display * 255), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                point_cloud = utils.point_cloud_from_depth(depth_array, color_array, boundary_array,\n",
        "                                                           intrinsic_array, point_cloud_downsampling=1)\n",
        "                utils.write_point_cloud(str(log_root / \"{}.ply\".format(batch)), point_cloud)\n",
        "\n",
        "        tq.close()\n",
        "        writer.close()\n",
        "\n",
        "    elif phase == \"test\":\n",
        "        test_dataset = dataset.SfMDataset(image_file_names=test_filenames,\n",
        "                                          folder_list=folder_list,\n",
        "                                          adjacent_range=adjacent_range, transform=None,\n",
        "                                          downsampling=input_downsampling,\n",
        "                                          network_downsampling=network_downsampling,\n",
        "                                          inlier_percentage=inlier_percentage,\n",
        "                                          use_store_data=load_intermediate_data,\n",
        "                                          store_data_root=evaluation_data_root,\n",
        "                                          phase=\"test\", is_hsv=is_hsv,\n",
        "                                          num_pre_workers=num_pre_workers, visible_interval=visibility_overlap,\n",
        "                                          rgb_mode=\"rgb\")\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False,\n",
        "                                                  num_workers=0)\n",
        "        depth_estimation_model = models.FCDenseNet57(n_classes=1)\n",
        "        # Initialize the depth estimation network with Kaiming He initialization\n",
        "        utils.init_net(depth_estimation_model, type=\"kaiming\", mode=\"fan_in\", activation_mode=\"relu\",\n",
        "                       distribution=\"normal\")\n",
        "        # Multi-GPU running\n",
        "        depth_estimation_model = torch.nn.DataParallel(depth_estimation_model)\n",
        "        # Summary network architecture\n",
        "        if display_architecture:\n",
        "            torchsummary.summary(depth_estimation_model, input_size=(3, height, width))\n",
        "\n",
        "        # Load trained model\n",
        "        if trained_model_path.exists():\n",
        "            print(\"Loading {:s} ...\".format(str(trained_model_path)))\n",
        "            state = torch.load(str(trained_model_path))\n",
        "            step = state['step']\n",
        "            epoch = state['epoch']\n",
        "            depth_estimation_model.load_state_dict(state['model'])\n",
        "            print('Restored model, epoch {}, step {}'.format(epoch, step))\n",
        "        else:\n",
        "            print(\"Trained model does not exist\")\n",
        "            raise OSError\n",
        "        depth_estimation_model = depth_estimation_model.module\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Set model to evaluation mode\n",
        "            depth_estimation_model.eval()\n",
        "            # Update progress bar\n",
        "            tq = tqdm.tqdm(total=len(test_loader) * batch_size)\n",
        "            for batch, (colors_1, boundaries, intrinsics, names) in enumerate(test_loader):\n",
        "                colors_1 = colors_1.cuda()\n",
        "                boundaries = boundaries.cuda()\n",
        "\n",
        "                colors_1 = boundaries * colors_1\n",
        "                predicted_depth_maps_1 = depth_estimation_model(colors_1)\n",
        "\n",
        "                color_display = np.uint8(\n",
        "                    255 * (0.5 * colors_1[0].permute(1, 2, 0).data.cpu().numpy() + 0.5).reshape((height, width, 3)))\n",
        "                if is_hsv:\n",
        "                    color_display = cv2.cvtColor(color_display, cv2.COLOR_HSV2BGR_FULL)\n",
        "                else:\n",
        "                    color_display = cv2.cvtColor(color_display, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "                boundary = boundaries[0].data.cpu().numpy().reshape((height, width))\n",
        "                color_display = np.uint8(boundary.reshape((height, width, 1)) * color_display)\n",
        "                depth_map = (boundaries * predicted_depth_maps_1)[0].data.cpu().numpy().reshape((height, width))\n",
        "                depth_display = cv2.applyColorMap(np.uint8(255 * depth_map / np.max(depth_map)), cv2.COLORMAP_JET)\n",
        "                point_cloud = utils.point_cloud_from_depth(depth_map=depth_map, color_img=color_display,\n",
        "                                                           mask_img=boundary,\n",
        "                                                           intrinsic_matrix=intrinsics[0].data.numpy(),\n",
        "                                                           point_cloud_downsampling=1)\n",
        "                utils.write_point_cloud(path=log_root / \"{}.ply\".format(names[0]), point_cloud=point_cloud)\n",
        "                cv2.imwrite(str(log_root / \"{}.png\".format(names[0])), cv2.hconcat([color_display, depth_display]))\n",
        "                tq.update(batch_size)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}