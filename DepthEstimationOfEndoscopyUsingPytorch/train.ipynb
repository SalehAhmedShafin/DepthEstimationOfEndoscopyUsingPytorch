{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "75c891f8",
      "metadata": {
        "id": "75c891f8"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import torchsummary\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "from tensorboardX import SummaryWriter\n",
        "import albumentations as albu\n",
        "import argparse\n",
        "import datetime\n",
        "import models\n",
        "import losses\n",
        "import utils\n",
        "import dataset\n",
        "import scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "KqWF3zi1HX1W"
      },
      "id": "KqWF3zi1HX1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plyfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6aw6muzSEev",
        "outputId": "e25f854a-df15-483a-891a-02692038bd9c"
      },
      "id": "L6aw6muzSEev",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plyfile\n",
            "  Downloading plyfile-0.9-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from plyfile) (1.22.4)\n",
            "Installing collected packages: plyfile\n",
            "Successfully installed plyfile-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssldi838H89f",
        "outputId": "def74a28-46a0-4590-ddd8-a2ecb8fcdbf3"
      },
      "id": "Ssldi838H89f",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "XiF5o_BjIK5_"
      },
      "id": "XiF5o_BjIK5_",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14b0f54",
      "metadata": {
        "id": "c14b0f54"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    cv2.destroyAllWindows()\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Self-supervised Depth Estimation on Monocular Endoscopy Dataset -- Train',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('--adjacent_range', nargs='+', type=int, required=True,\n",
        "                        help='interval range for a pair of video frames')\n",
        "    parser.add_argument('--id_range', nargs='+', type=int, required=True,\n",
        "                        help='id range for the training and testing dataset')\n",
        "    parser.add_argument('--input_downsampling', type=float, default=4.0,\n",
        "                        help='image downsampling rate')\n",
        "    parser.add_argument('--input_size', nargs='+', type=int, required=True, help='resolution of network input')\n",
        "    parser.add_argument('--batch_size', type=int, default=8, help='batch size for training and testing')\n",
        "    parser.add_argument('--num_workers', type=int, default=8, help='number of workers for input data loader')\n",
        "    parser.add_argument('--num_pre_workers', type=int, default=8,\n",
        "                        help='number of workers for preprocessing intermediate data')\n",
        "    parser.add_argument('--dcl_weight', type=float, default=5.0,\n",
        "                        help='weight for depth consistency loss in the later training stage')\n",
        "    parser.add_argument('--sfl_weight', type=float, default=20.0, help='weight for sparse flow loss')\n",
        "    parser.add_argument('--max_lr', type=float, default=1.0e-3, help='upper bound learning rate for cyclic lr')\n",
        "    parser.add_argument('--min_lr', type=float, default=1.0e-4, help='lower bound learning rate for cyclic lr')\n",
        "    parser.add_argument('--num_iter', type=int, default=1000, help='number of iterations per epoch')\n",
        "    parser.add_argument('--network_downsampling', type=int, default=64, help='network downsampling of the input image')\n",
        "    parser.add_argument('--inlier_percentage', type=float, default=0.99,\n",
        "                        help='percentage of inliers of SfM point clouds (for pruning some outliers)')\n",
        "    parser.add_argument('--validation_interval', type=int, default=1, help='epoch interval for validation')\n",
        "    parser.add_argument('--zero_division_epsilon', type=float, default=1.0e-8, help='epsilon to prevent zero division')\n",
        "    parser.add_argument('--display_interval', type=int, default=10, help='iteration interval for image display')\n",
        "    parser.add_argument('--training_patient_id', nargs='+', type=int, required=True, help='id of the training patient')\n",
        "    parser.add_argument('--testing_patient_id', nargs='+', type=int, required=True, help='id of the testing patient')\n",
        "    parser.add_argument('--validation_patient_id', nargs='+', type=int, required=True,\n",
        "                        help='id of the valiadtion patient')\n",
        "    parser.add_argument('--load_intermediate_data', action='store_true', help='whether to load intermediate data')\n",
        "    parser.add_argument('--load_trained_model', action='store_true',\n",
        "                        help='whether to load trained student model')\n",
        "    parser.add_argument('--number_epoch', type=int, required=True, help='number of epochs in total')\n",
        "    parser.add_argument('--visibility_overlap', type=int, default=30, help='overlap of point visibility information')\n",
        "    parser.add_argument('--use_hsv_colorspace', action='store_true',\n",
        "                        help='convert RGB to hsv colorspace')\n",
        "    parser.add_argument('--training_result_root', type=str, required=True, help='root of the training input and ouput')\n",
        "    parser.add_argument('--training_data_root', type=str, required=True, help='path to the training data')\n",
        "    parser.add_argument('--architecture_summary', action='store_true', help='display the network architecture')\n",
        "    parser.add_argument('--trained_model_path', type=str, default=None,\n",
        "                        help='path to the trained student model')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.manual_seed(10085)\n",
        "    np.random.seed(10085)\n",
        "    random.seed(10085)\n",
        "\n",
        "    adjacent_range = args.adjacent_range\n",
        "    input_downsampling = args.input_downsampling\n",
        "    height, width = args.input_size\n",
        "    batch_size = args.batch_size\n",
        "    num_workers = args.num_workers\n",
        "    num_pre_workers = args.num_pre_workers\n",
        "    depth_consistency_weight = args.dcl_weight\n",
        "    sparse_flow_weight = args.sfl_weight\n",
        "    max_lr = args.max_lr\n",
        "    min_lr = args.min_lr\n",
        "    num_iter = args.num_iter\n",
        "    network_downsampling = args.network_downsampling\n",
        "    inlier_percentage = args.inlier_percentage\n",
        "    validation_each = args.validation_interval\n",
        "    depth_scaling_epsilon = args.zero_division_epsilon\n",
        "    depth_warping_epsilon = args.zero_division_epsilon\n",
        "    wsl_epsilon = args.zero_division_epsilon\n",
        "    display_each = args.display_interval\n",
        "    training_patient_id = args.training_patient_id\n",
        "    testing_patient_id = args.testing_patient_id\n",
        "    validation_patient_id = args.validation_patient_id\n",
        "    load_intermediate_data = args.load_intermediate_data\n",
        "    load_trained_model = args.load_trained_model\n",
        "    n_epochs = args.number_epoch\n",
        "    is_hsv = args.use_hsv_colorspace\n",
        "    training_result_root = args.training_result_root\n",
        "    display_architecture = args.architecture_summary\n",
        "    trained_model_path = args.trained_model_path\n",
        "    training_data_root = Path(args.training_data_root)\n",
        "    id_range = args.id_range\n",
        "    visibility_overlap = args.visibility_overlap\n",
        "    currentDT = datetime.datetime.now()\n",
        "\n",
        "    depth_estimation_model_teacher = []\n",
        "    failure_sequences = []\n",
        "\n",
        "    training_transforms = albu.Compose([\n",
        "        albu.OneOf([\n",
        "            albu.Compose([\n",
        "                albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "                albu.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "                albu.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=0, val_shift_limit=0, p=0.5)]),\n",
        "            albu.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=30, val_shift_limit=30, p=0.5)\n",
        "        ]),\n",
        "        albu.OneOf([\n",
        "            albu.Blur(p=0.5),\n",
        "            albu.MedianBlur(p=0.5),\n",
        "            albu.MotionBlur(p=0.5),\n",
        "            albu.JpegCompression(quality_lower=20, quality_upper=100, p=0.5)\n",
        "        ]),\n",
        "        albu.OneOf([\n",
        "            albu.GaussNoise(var_limit=(10, 30), p=0.5),\n",
        "            albu.IAAAdditiveGaussianNoise(loc=0, scale=(0.005 * 255, 0.02 * 255), p=0.5)\n",
        "        ]),\n",
        "    ], p=1.)\n",
        "\n",
        "    log_root = Path(training_result_root) / \"depth_estimation_train_run_{}_{}_{}_{}_test_id_{}\".format(\n",
        "        currentDT.month,\n",
        "        currentDT.day,\n",
        "        currentDT.hour,\n",
        "        currentDT.minute,\n",
        "        \"_\".join(testing_patient_id))\n",
        "    if not log_root.exists():\n",
        "        log_root.mkdir()\n",
        "    writer = SummaryWriter(logdir=str(log_root))\n",
        "    print(\"Tensorboard shows at {}\".format(str(log_root)))\n",
        "\n",
        "    train_filenames, val_filenames, test_filenames = utils.get_color_file_names_by_bag(training_data_root,\n",
        "                                                                                       training_patient_id=training_patient_id,\n",
        "                                                                                       validation_patient_id=validation_patient_id,\n",
        "                                                                                       testing_patient_id=testing_patient_id)\n",
        "    folder_list = utils.get_parent_folder_names(training_data_root, id_range=id_range)\n",
        "\n",
        "    train_dataset = dataset.SfMDataset(image_file_names=train_filenames,\n",
        "                                       folder_list=folder_list,\n",
        "                                       adjacent_range=adjacent_range, transform=training_transforms,\n",
        "                                       downsampling=input_downsampling,\n",
        "                                       network_downsampling=network_downsampling, inlier_percentage=inlier_percentage,\n",
        "                                       use_store_data=load_intermediate_data,\n",
        "                                       store_data_root=training_data_root,\n",
        "                                       phase=\"train\", is_hsv=is_hsv,\n",
        "                                       num_pre_workers=num_pre_workers, visible_interval=visibility_overlap,\n",
        "                                       rgb_mode=\"rgb\", num_iter=num_iter)\n",
        "    validation_dataset = dataset.SfMDataset(image_file_names=val_filenames,\n",
        "                                            folder_list=folder_list,\n",
        "                                            adjacent_range=adjacent_range,\n",
        "                                            transform=None,\n",
        "                                            downsampling=input_downsampling,\n",
        "                                            network_downsampling=network_downsampling,\n",
        "                                            inlier_percentage=inlier_percentage,\n",
        "                                            use_store_data=True,\n",
        "                                            store_data_root=training_data_root,\n",
        "                                            phase=\"validation\", is_hsv=is_hsv,\n",
        "                                            num_pre_workers=num_pre_workers, visible_interval=visibility_overlap,\n",
        "                                            rgb_mode=\"rgb\", num_iter=None)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=num_workers)\n",
        "    validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                                    num_workers=batch_size)\n",
        "\n",
        "    depth_estimation_model = models.FCDenseNet57(n_classes=1)\n",
        "    depth_estimation_model = utils.init_net(depth_estimation_model, type=\"kaiming\", mode=\"fan_in\",\n",
        "                                            activation_mode=\"relu\",\n",
        "                                            distribution=\"normal\")\n",
        "    depth_estimation_model = torch.nn.DataParallel(depth_estimation_model)\n",
        "    if display_architecture:\n",
        "        torchsummary.summary(depth_estimation_model, input_size=(3, height, width))\n",
        "    optimizer = torch.optim.SGD(depth_estimation_model.parameters(), lr=max_lr, momentum=0.9)\n",
        "    lr_scheduler = scheduler.CyclicLR(optimizer, base_lr=min_lr, max_lr=max_lr, step_size=num_iter)\n",
        "\n",
        "    depth_scaling_layer = models.DepthScalingLayer(epsilon=depth_scaling_epsilon)\n",
        "    depth_warping_layer = models.DepthWarpingLayer(epsilon=depth_warping_epsilon)\n",
        "    flow_from_depth_layer = models.FlowfromDepthLayer()\n",
        "    sparse_flow_loss_function = losses.SparseMaskedL1Loss()\n",
        "    depth_consistency_loss_function = losses.NormalizedDistanceLoss(height=height, width=width)\n",
        "    if load_trained_model:\n",
        "        if Path(trained_model_path).exists():\n",
        "            print(\"Loading {:s} ...\".format(trained_model_path))\n",
        "            state = torch.load(trained_model_path)\n",
        "            step = state['step']\n",
        "            epoch = state['epoch']\n",
        "            depth_estimation_model.load_state_dict(state['model'])\n",
        "            print('Restored model, epoch {}, step {}'.format(epoch, step))\n",
        "        else:\n",
        "            print(\"No trained model detected\")\n",
        "            raise OSError\n",
        "    else:\n",
        "        epoch = 0\n",
        "        step = 0\n",
        "\n",
        "    for epoch in range(epoch, n_epochs + 1):\n",
        "      \n",
        "        torch.manual_seed(10086 + epoch)\n",
        "        np.random.seed(10086 + epoch)\n",
        "        random.seed(10086 + epoch)\n",
        "        depth_estimation_model.train()\n",
        "\n",
        "        tq = tqdm.tqdm(total=len(train_loader) * batch_size, dynamic_ncols=True, ncols=40)\n",
        "        if epoch <= 25:\n",
        "            depth_consistency_weight = 0.1\n",
        "        else:\n",
        "            depth_consistency_weight = args.dcl_weight\n",
        "\n",
        "        for batch, (\n",
        "                colors_1, colors_2, sparse_depths_1, sparse_depths_2, sparse_depth_masks_1, sparse_depth_masks_2,\n",
        "                sparse_flows_1, sparse_flows_2, sparse_flow_masks_1, sparse_flow_masks_2, boundaries, rotations_1_wrt_2,\n",
        "                rotations_2_wrt_1, translations_1_wrt_2, translations_2_wrt_1, intrinsics, folders, file_names) in \\\n",
        "                enumerate(train_loader):\n",
        "\n",
        "            lr_scheduler.batch_step(batch_iteration=step)\n",
        "            tq.set_description('Epoch {}, lr {}'.format(epoch, lr_scheduler.get_lr()))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                colors_1 = colors_1.cuda()\n",
        "                colors_2 = colors_2.cuda()\n",
        "                sparse_depths_1 = sparse_depths_1.cuda()\n",
        "                sparse_depths_2 = sparse_depths_2.cuda()\n",
        "                sparse_depth_masks_1 = sparse_depth_masks_1.cuda()\n",
        "                sparse_depth_masks_2 = sparse_depth_masks_2.cuda()\n",
        "                sparse_flows_1 = sparse_flows_1.cuda()\n",
        "                sparse_flows_2 = sparse_flows_2.cuda()\n",
        "                sparse_flow_masks_1 = sparse_flow_masks_1.cuda()\n",
        "                sparse_flow_masks_2 = sparse_flow_masks_2.cuda()\n",
        "                boundaries = boundaries.cuda()\n",
        "                rotations_1_wrt_2 = rotations_1_wrt_2.cuda()\n",
        "                rotations_2_wrt_1 = rotations_2_wrt_1.cuda()\n",
        "                translations_1_wrt_2 = translations_1_wrt_2.cuda()\n",
        "                translations_2_wrt_1 = translations_2_wrt_1.cuda()\n",
        "                intrinsics = intrinsics.cuda()\n",
        "\n",
        "            colors_1 = boundaries * colors_1\n",
        "            colors_2 = boundaries * colors_2\n",
        "            predicted_depth_maps_1 = depth_estimation_model(colors_1)\n",
        "            predicted_depth_maps_2 = depth_estimation_model(colors_2)\n",
        "\n",
        "            scaled_depth_maps_1, normalized_scale_std_1 = depth_scaling_layer(\n",
        "                [predicted_depth_maps_1, sparse_depths_1, sparse_depth_masks_1])\n",
        "            scaled_depth_maps_2, normalized_scale_std_2 = depth_scaling_layer(\n",
        "                [predicted_depth_maps_2, sparse_depths_2, sparse_depth_masks_2])\n",
        "\n",
        "           \n",
        "            flows_from_depth_1 = flow_from_depth_layer(\n",
        "                [scaled_depth_maps_1, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                 intrinsics])\n",
        "            flows_from_depth_2 = flow_from_depth_layer(\n",
        "                [scaled_depth_maps_2, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                 intrinsics])\n",
        "            sparse_flow_masks_1 = sparse_flow_masks_1 * boundaries\n",
        "            sparse_flow_masks_2 = sparse_flow_masks_2 * boundaries\n",
        "            sparse_flows_1 = sparse_flows_1 * boundaries\n",
        "            sparse_flows_2 = sparse_flows_2 * boundaries\n",
        "            flows_from_depth_1 = flows_from_depth_1 * boundaries\n",
        "            flows_from_depth_2 = flows_from_depth_2 * boundaries\n",
        "\n",
        "            sparse_flow_loss = sparse_flow_weight * 0.5 * (sparse_flow_loss_function(\n",
        "                [sparse_flows_1, flows_from_depth_1, sparse_flow_masks_1]) + sparse_flow_loss_function(\n",
        "                [sparse_flows_2, flows_from_depth_2, sparse_flow_masks_2]))\n",
        "\n",
        "            warped_depth_maps_2_to_1, intersect_masks_1 = depth_warping_layer(\n",
        "                [scaled_depth_maps_1, scaled_depth_maps_2, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                 intrinsics])\n",
        "            warped_depth_maps_1_to_2, intersect_masks_2 = depth_warping_layer(\n",
        "                [scaled_depth_maps_2, scaled_depth_maps_1, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                 intrinsics])\n",
        "            depth_consistency_loss = depth_consistency_weight * 0.5 * (depth_consistency_loss_function(\n",
        "                [scaled_depth_maps_1, warped_depth_maps_2_to_1, intersect_masks_1,\n",
        "                 intrinsics]) + depth_consistency_loss_function(\n",
        "                [scaled_depth_maps_2, warped_depth_maps_1_to_2, intersect_masks_2, intrinsics]))\n",
        "            loss = depth_consistency_loss + sparse_flow_loss\n",
        "\n",
        "            if math.isnan(loss.item()) or math.isinf(loss.item()):\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.zero_grad()\n",
        "                optimizer.step()\n",
        "                continue\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # Prevent one sample from having too much impact on the training\n",
        "                torch.nn.utils.clip_grad_norm_(depth_estimation_model.parameters(), 10.0)\n",
        "                optimizer.step()\n",
        "                if batch == 0:\n",
        "                    mean_loss = loss.item()\n",
        "                    mean_depth_consistency_loss = depth_consistency_loss.item()\n",
        "                    mean_sparse_flow_loss = sparse_flow_loss.item()\n",
        "                else:\n",
        "                    mean_loss = (mean_loss * batch + loss.item()) / (batch + 1.0)\n",
        "                    mean_depth_consistency_loss = (mean_depth_consistency_loss * batch +\n",
        "                                                   depth_consistency_loss.item()) / (batch + 1.0)\n",
        "                    mean_sparse_flow_loss = (mean_sparse_flow_loss * batch + sparse_flow_loss.item()) / (batch + 1.0)\n",
        "\n",
        "            step += 1\n",
        "            tq.update(batch_size)\n",
        "            tq.set_postfix(loss='avg: {:.5f} cur: {:.5f}'.format(mean_loss, loss.item()),\n",
        "                           loss_depth_consistency='avg: {:.5f} cur: {:.5f}'.format(\n",
        "                               mean_depth_consistency_loss,\n",
        "                               depth_consistency_loss.item()),\n",
        "                           loss_sparse_flow='avg: {:.5f} cur: {:.5f}'.format(\n",
        "                               mean_sparse_flow_loss,\n",
        "                               sparse_flow_loss.item()))\n",
        "            writer.add_scalars('Training', {'overall': mean_loss,\n",
        "                                            'depth_consistency': mean_depth_consistency_loss,\n",
        "                                            'sparse_flow': mean_sparse_flow_loss}, step)\n",
        "\n",
        "            if batch % display_each == 0:\n",
        "                colors_1_display, pred_depths_1_display, sparse_flows_1_display, dense_flows_1_display = \\\n",
        "                    utils.display_color_depth_sparse_flow_dense_flow(1, step, writer, colors_1,\n",
        "                                                                     scaled_depth_maps_1 * boundaries,\n",
        "                                                                     sparse_flows_1, flows_from_depth_1, is_hsv,\n",
        "                                                                     phase=\"Training\", is_return_image=True,\n",
        "                                                                     color_reverse=True)\n",
        "                colors_2_display, pred_depths_2_display, sparse_flows_2_display, dense_flows_2_display = \\\n",
        "                    utils.display_color_depth_sparse_flow_dense_flow(2, step, writer, colors_2,\n",
        "                                                                     scaled_depth_maps_2 * boundaries,\n",
        "                                                                     sparse_flows_2, flows_from_depth_2, is_hsv,\n",
        "                                                                     phase=\"Training\", is_return_image=True,\n",
        "                                                                     color_reverse=True)\n",
        "                utils.stack_and_display(phase=\"Training\", title=\"Results (c1, d1, sf1, df1, c2, d2, sf2, df2)\",\n",
        "                                        step=step, writer=writer,\n",
        "                                        image_list=[colors_1_display, pred_depths_1_display, sparse_flows_1_display,\n",
        "                                                    dense_flows_1_display,\n",
        "                                                    colors_2_display, pred_depths_2_display, sparse_flows_2_display,\n",
        "                                                    dense_flows_2_display])\n",
        "        tq.close()\n",
        "\n",
        "        if epoch % validation_each != 0:\n",
        "            continue\n",
        "\n",
        "        tq = tqdm.tqdm(total=len(validation_loader) * batch_size, dynamic_ncols=True, ncols=40)\n",
        "        tq.set_description('Validation Epoch {}'.format(epoch))\n",
        "        with torch.no_grad():\n",
        "            for batch, (\n",
        "                    colors_1, colors_2, sparse_depths_1, sparse_depths_2, sparse_depth_masks_1,\n",
        "                    sparse_depth_masks_2, sparse_flows_1,\n",
        "                    sparse_flows_2, sparse_flow_masks_1, sparse_flow_masks_2, boundaries, rotations_1_wrt_2,\n",
        "                    rotations_2_wrt_1, translations_1_wrt_2, translations_2_wrt_1, intrinsics,\n",
        "                    folders, file_names) in enumerate(validation_loader):\n",
        "\n",
        "                colors_1 = colors_1.cuda()\n",
        "                colors_2 = colors_2.cuda()\n",
        "                sparse_depths_1 = sparse_depths_1.cuda()\n",
        "                sparse_depths_2 = sparse_depths_2.cuda()\n",
        "                sparse_depth_masks_1 = sparse_depth_masks_1.cuda()\n",
        "                sparse_depth_masks_2 = sparse_depth_masks_2.cuda()\n",
        "                sparse_flows_1 = sparse_flows_1.cuda()\n",
        "                sparse_flows_2 = sparse_flows_2.cuda()\n",
        "                sparse_flow_masks_1 = sparse_flow_masks_1.cuda()\n",
        "                sparse_flow_masks_2 = sparse_flow_masks_2.cuda()\n",
        "                boundaries = boundaries.cuda()\n",
        "                rotations_1_wrt_2 = rotations_1_wrt_2.cuda()\n",
        "                rotations_2_wrt_1 = rotations_2_wrt_1.cuda()\n",
        "                translations_1_wrt_2 = translations_1_wrt_2.cuda()\n",
        "                translations_2_wrt_1 = translations_2_wrt_1.cuda()\n",
        "                intrinsics = intrinsics.cuda()\n",
        "\n",
        "                colors_1 = boundaries * colors_1\n",
        "                colors_2 = boundaries * colors_2\n",
        "\n",
        "                predicted_depth_maps_1 = depth_estimation_model(colors_1)\n",
        "                predicted_depth_maps_2 = depth_estimation_model(colors_2)\n",
        "                scaled_depth_maps_1, normalized_scale_std_1 = depth_scaling_layer(\n",
        "                    [torch.abs(predicted_depth_maps_1), sparse_depths_1, sparse_depth_masks_1])\n",
        "                scaled_depth_maps_2, normalized_scale_std_2 = depth_scaling_layer(\n",
        "                    [torch.abs(predicted_depth_maps_2), sparse_depths_2, sparse_depth_masks_2])\n",
        "\n",
        "        \n",
        "                flows_from_depth_1 = flow_from_depth_layer(\n",
        "                    [scaled_depth_maps_1, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                     intrinsics])\n",
        "                flows_from_depth_2 = flow_from_depth_layer(\n",
        "                    [scaled_depth_maps_2, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                     intrinsics])\n",
        "                sparse_flow_masks_1 = sparse_flow_masks_1 * boundaries\n",
        "                sparse_flow_masks_2 = sparse_flow_masks_2 * boundaries\n",
        "                sparse_flows_1 = sparse_flows_1 * boundaries\n",
        "                sparse_flows_2 = sparse_flows_2 * boundaries\n",
        "                flows_from_depth_1 = flows_from_depth_1 * boundaries\n",
        "                flows_from_depth_2 = flows_from_depth_2 * boundaries\n",
        "                sparse_flow_loss = sparse_flow_weight * 0.5 * (sparse_flow_loss_function(\n",
        "                    [sparse_flows_1, flows_from_depth_1, sparse_flow_masks_1]) + sparse_flow_loss_function(\n",
        "                    [sparse_flows_2, flows_from_depth_2, sparse_flow_masks_2]))\n",
        "\n",
        "                warped_depth_maps_2_to_1, intersect_masks_1 = depth_warping_layer(\n",
        "                    [scaled_depth_maps_1, scaled_depth_maps_2, boundaries, translations_1_wrt_2, rotations_1_wrt_2,\n",
        "                     intrinsics])\n",
        "                warped_depth_maps_1_to_2, intersect_masks_2 = depth_warping_layer(\n",
        "                    [scaled_depth_maps_2, scaled_depth_maps_1, boundaries, translations_2_wrt_1, rotations_2_wrt_1,\n",
        "                     intrinsics])\n",
        "                depth_consistency_loss = depth_consistency_weight * 0.5 * (depth_consistency_loss_function(\n",
        "                    [scaled_depth_maps_1, warped_depth_maps_2_to_1,\n",
        "                     intersect_masks_1, intrinsics]) + depth_consistency_loss_function(\n",
        "                    [scaled_depth_maps_2, warped_depth_maps_1_to_2, intersect_masks_2, intrinsics]))\n",
        "\n",
        "                loss = depth_consistency_loss + sparse_flow_loss\n",
        "                tq.update(batch_size)\n",
        "                if not np.isnan(loss.item()):\n",
        "                    if batch == 0:\n",
        "                        mean_loss = loss.item()\n",
        "                        mean_depth_consistency_loss = depth_consistency_loss.item()\n",
        "                        mean_sparse_flow_loss = sparse_flow_loss.item()\n",
        "                    else:\n",
        "                        mean_loss = (mean_loss * batch + loss.item()) / (batch + 1.0)\n",
        "                        mean_depth_consistency_loss = (mean_depth_consistency_loss * batch +\n",
        "                                                       depth_consistency_loss.item()) / (batch + 1.0)\n",
        "                        mean_sparse_flow_loss = (mean_sparse_flow_loss * batch + sparse_flow_loss.item()) / (\n",
        "                                batch + 1.0)\n",
        "\n",
        "                if batch % display_each == 0:\n",
        "                    colors_1_display, pred_depths_1_display, sparse_flows_1_display, dense_flows_1_display = \\\n",
        "                        utils.display_color_depth_sparse_flow_dense_flow(1, step, writer, colors_1,\n",
        "                                                                         scaled_depth_maps_1 * boundaries,\n",
        "                                                                         sparse_flows_1, flows_from_depth_1, is_hsv,\n",
        "                                                                         phase=\"Validation\", is_return_image=True,\n",
        "                                                                         color_reverse=True)\n",
        "                    colors_2_display, pred_depths_2_display, sparse_flows_2_display, dense_flows_2_display = \\\n",
        "                        utils.display_color_depth_sparse_flow_dense_flow(2, step, writer, colors_2,\n",
        "                                                                         scaled_depth_maps_2 * boundaries,\n",
        "                                                                         sparse_flows_2, flows_from_depth_2, is_hsv,\n",
        "                                                                         phase=\"Validation\", is_return_image=True,\n",
        "                                                                         color_reverse=True)\n",
        "                    utils.stack_and_display(phase=\"Validation\", title=\"Results (c1, d1, sf1, df1, c2, d2, sf2, df2)\",\n",
        "                                            step=step, writer=writer,\n",
        "                                            image_list=[colors_1_display, pred_depths_1_display, sparse_flows_1_display,\n",
        "                                                        dense_flows_1_display,\n",
        "                                                        colors_2_display, pred_depths_2_display, sparse_flows_2_display,\n",
        "                                                        dense_flows_2_display])\n",
        "\n",
        "                # TensorboardX\n",
        "                writer.add_scalars('Validation', {'overall': mean_loss,\n",
        "                                                  'depth_consistency': mean_depth_consistency_loss,\n",
        "                                                  'sparse_flow': mean_sparse_flow_loss}, epoch)\n",
        "\n",
        "        tq.close()\n",
        "        model_path_epoch = log_root / 'checkpoint_model_epoch_{}_validation_{}.pt'.format(epoch,\n",
        "                                                                                          mean_sparse_flow_loss)\n",
        "        utils.save_model(model=depth_estimation_model, optimizer=optimizer,\n",
        "                         epoch=epoch + 1, step=step, model_path=model_path_epoch,\n",
        "                         validation_loss=mean_sparse_flow_loss)\n",
        "        writer.export_scalars_to_json(\n",
        "            str(log_root / (\"all_scalars_\" + str(epoch) + \".json\")))\n",
        "\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c4795db",
      "metadata": {
        "id": "2c4795db"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}